# training/run_grpo_trl.py
import glob
import os
import math
import numpy as np
# ==== åœ¨ä»»ä½•ç”¨åˆ° transformers ä¹‹å‰æ‰“è¡¥ä¸ ====
import transformers
from transformers.utils import import_utils

def _disable_torch_load_check(*args, **kwargs):
    # è¯¾ç¨‹é¡¹ç›®ç”¨çš„ä¸´æ—¶è¡¥ä¸ï¼šå…³é—­ torch>=2.6 å¼ºåˆ¶æ£€æŸ¥
    # æ³¨æ„åªåŠ è½½æ¥è‡ª HuggingFace å®˜æ–¹æˆ–å¯ä¿¡ä½œè€…çš„æƒé‡
    return

# 1) æ”¹ import_utils é‡Œçš„å®ç°
import_utils.check_torch_load_is_safe = _disable_torch_load_check

# 2) åŒæ—¶æ”¹ modeling_utils é‡Œæ‹¿åˆ°çš„åˆ«å
try:
    from transformers import modeling_utils
    if hasattr(modeling_utils, "check_torch_load_is_safe"):
        modeling_utils.check_torch_load_is_safe = _disable_torch_load_check
except Exception:
    # ä¸‡ä¸€ä¸åŒç‰ˆæœ¬å¯¼å…¥æ–¹å¼ä¸ä¸€æ ·ï¼Œè¿™é‡Œå°±é™é»˜è·³è¿‡
    pass

# 3) å…³é”®ï¼šæ”¹ trainer æ¨¡å—é‡Œçš„æœ¬åœ°å¼•ç”¨
try:
    import transformers.trainer as trainer_mod
    if hasattr(trainer_mod, "check_torch_load_is_safe"):
        trainer_mod.check_torch_load_is_safe = _disable_torch_load_check
except Exception:
    pass
# ==========================================
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    BitsAndBytesConfig,
)

import torch
import torch.nn.functional as F
from peft import PeftModel
from datasets import Dataset
from trl import GRPOTrainer, GRPOConfig

from data_utils.dataset_build import load_split_raw_data
from data_utils.prompts import generate_test_prompt

###############################################
# 1. åˆ†ç±» helperï¼šä» LLaMA å¾—åˆ°ä¸‰åˆ†ç±»æ¦‚ç‡
###############################################

LABEL_ORDER = ["negative", "neutral", "positive"]


def get_label_token_ids(tokenizer, label_order):
    """
    label_order: æ¯”å¦‚ ["negative", "neutral", "positive"]
    è¿”å›: dict[label] -> token_ids (list[int])
    """
    label_token_ids = {}
    for label in label_order:
        ids = tokenizer.encode(label, add_special_tokens=False)
        label_token_ids[label] = ids
    return label_token_ids

@torch.no_grad()
def get_student_probs_from_prompts(model, tokenizer, prompts, label_order, max_length: int = 512):
    """
    ç»™å®šä¸€æ‰¹ promptï¼Œç”¨å½“å‰å­¦ç”Ÿæ¨¡å‹ç®—å‡ºå¯¹æ¯ä¸ªæƒ…æ„Ÿæ ‡ç­¾çš„æ¦‚ç‡ã€‚
    è¿™é‡Œç”¨çš„æ˜¯ã€Œä¸‹ä¸€ tokenã€çš„ logitsï¼Œå¯¹æ¯ä¸ª label çš„ token é›†åˆåš log-sum-exp ä¹‹å softmaxã€‚
    """
    device = next(model.parameters()).device

    toks = tokenizer(
        prompts,
        padding=True,
        truncation=True,
        return_tensors="pt",
        max_length=max_length,
    ).to(device)

    out = model(**toks)
    # æ¯ä¸ªæ ·æœ¬æœ€åä¸€ä¸ªé padding token çš„ index
    last_idx = toks["attention_mask"].sum(dim=1) - 1
    logits_last = out.logits[torch.arange(out.logits.size(0)), last_idx, :]

    label_token_ids = get_label_token_ids(tokenizer, label_order)
    label_scores = []
    for label in label_order:
        ids = label_token_ids[label]
        if len(ids) == 1:
            score = logits_last[:, ids[0]]
        else:
            # å¤š token çš„ labelï¼šå¯¹å¯¹åº” token çš„ logits åš log-sum-exp
            score = torch.logsumexp(logits_last[:, ids], dim=1)
        label_scores.append(score)

    scores = torch.stack(label_scores, dim=1)  # [B, num_labels]
    probs = scores.softmax(dim=1)
    return probs

############################################################
# 2. Dataset è½¬æ¢ï¼šX_train_raw(DataFrame) â†’ HF Dataset
############################################################

def convert_to_hf_dataset(X_train_raw):
    """
    X_train_raw: DataFrameï¼ŒåŒ…å«åˆ— ["text", "sentiment", "orig_text", "pert_text"]

    è¾“å‡ºå­—æ®µï¼š
        - prompt:       åŸå§‹æ–‡æœ¬ orig_text ç”Ÿæˆçš„ prompt
        - pert_prompt:  æ‰°åŠ¨æ–‡æœ¬ pert_text ç”Ÿæˆçš„ promptï¼ˆè‹¥æ— æ‰°åŠ¨ï¼Œåˆ™ä¸º Noneï¼‰
        - ground_truth: sentiment
        - orig_text, pert_text: ä¿ç•™åŸæ–‡ï¼Œè°ƒè¯•/åˆ†æç”¨
    """

    def process(row):
        label = row["sentiment"]
        orig = row.get("orig_text", row["text"])
        pert = row.get("pert_text", None)

        # ä¿è¯ä¼ ç»™ generate_test_prompt çš„ row ç»“æ„ä¸ä½ ä¹‹å‰ä¸€æ ·
        row_orig = dict(row)
        row_orig["text"] = orig
        row_orig["sentiment"] = label
        prompt = generate_test_prompt(row_orig)

        pert_prompt = None
        if pert is not None:
            row_pert = dict(row)
            row_pert["text"] = pert
            row_pert["sentiment"] = label
            pert_prompt = generate_test_prompt(row_pert)

        return {
            "prompt": prompt,
            "pert_prompt": pert_prompt,
            "ground_truth": label,
            "orig_text": orig,
            "pert_text": pert,
        }

    hf_ds = Dataset.from_pandas(X_train_raw.reset_index(drop=True))
    hf_ds = hf_ds.map(process)
    return hf_ds

##############################################
# 3. åŸºç¡€ rewardï¼šæ ¼å¼ + ä¸¥æ ¼å‡†ç¡®ç‡
##############################################

def _extract_text_from_completion(completion):
    """å…¼å®¹ GRPO è¿”å›çš„å¤šç§ completion ç»“æ„."""
    # å¯èƒ½æ˜¯çº¯å­—ç¬¦ä¸²
    if isinstance(completion, str):
        return completion
    # å¯èƒ½æ˜¯ list[{"role": "...", "content": "..."}]
    if isinstance(completion, list) and completion:
        last = completion[-1]
        if isinstance(last, dict):
            return str(last.get("content", ""))
        return str(last)
    # å…œåº•
    return str(completion)


def _extract_label_from_text(text: str, label_order=None):
    if label_order is None:
        label_order = LABEL_ORDER
    low = text.lower()
    for lab in label_order:
        if lab in low:
            return lab
    return None


######################################################################
# 4. æ‰°åŠ¨ä¸€è‡´æ€§ rewardï¼šclean vs perturbed å¯¹ç§° KL
######################################################################

def consistency_reward_base(
    prompts,
    model,
    tokenizer,
    label_order,
    pert_prompts_list,
):
    """
    å¯¹äºæ¯ä¸ªæœ‰æ‰°åŠ¨ç‰ˆæœ¬çš„æ ·æœ¬ï¼Œè®¡ç®—ï¼š
        sym_kl = KL(p(x) || p(x~)) + KL(p(x~) || p(x))
    ä½œä¸ºä¸€è‡´æ€§æƒ©ç½šçš„è´Ÿå·ï¼š reward = - sym_kl
    """
    pair_prompts = []
    pair_pert_prompts = []
    idx_map = []

    for i, (p, pp) in enumerate(zip(prompts, pert_prompts_list)):
        if pp is None or pp == "":
            continue
        pair_prompts.append(p)
        pair_pert_prompts.append(pp)
        idx_map.append(i)

    if not pair_prompts:
        return [0.0] * len(prompts)

    with torch.no_grad():
        p_x = get_student_probs_from_prompts(
            model, tokenizer, pair_prompts, label_order
        )
        p_xt = get_student_probs_from_prompts(
            model, tokenizer, pair_pert_prompts, label_order
        )

        kl1 = F.kl_div(
            p_x.clamp_min(1e-12).log(),
            p_xt.clamp_min(1e-12),
            reduction="none",
        ).sum(dim=-1)
        kl2 = F.kl_div(
            p_xt.clamp_min(1e-12).log(),
            p_x.clamp_min(1e-12),
            reduction="none",
        ).sum(dim=-1)

        sym_kl = kl1 + kl2
        vals = (-sym_kl).detach().cpu().tolist()

    rewards = [0.0] * len(prompts)
    for i, v in zip(idx_map, vals):
        rewards[i] = v
    return rewards

def build_finbert_teacher(finbert_model_name, label_order):
    """
    è¿”å›ä¸€ä¸ª teacher_probs_fn: texts -> [B, 3] æ¦‚ç‡ï¼ˆé¡ºåºä¸ label_order å¯¹é½ï¼‰
    FinBERT ä»é»˜è®¤ HF ç¼“å­˜ (~/.cache/huggingface/hub) åŠ è½½ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ cache_dirã€‚
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print(f"ğŸ”§ [FinBERT] å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½: {finbert_model_name}")
    try:
        finbert_tokenizer = AutoTokenizer.from_pretrained(
            finbert_model_name,
            local_files_only=True,   # â­ åªç”¨æœ¬åœ°ç¼“å­˜ï¼Œä¸è”ç½‘
        )
        finbert_model = AutoModelForSequenceClassification.from_pretrained(
            finbert_model_name,
            local_files_only=True,
        ).to(device)
        finbert_model.eval()
    except Exception as e:
        print("âŒ [FinBERT] åŠ è½½å¤±è´¥ï¼Œå°†ç¦ç”¨ FinBERT rewardã€‚")
        print("   Error:", repr(e))
        return None

    # æ ¹æ® FinBERT çš„ id2label è‡ªåŠ¨å¯¹é½åˆ° ["negative", "neutral", "positive"]
    id2label = {int(k): v.lower() for k, v in finbert_model.config.id2label.items()}
    index_order = []
    for lab in label_order:
        matched = [i for i, name in id2label.items() if lab in name]
        if not matched:
            raise ValueError(f"Cannot find label containing '{lab}' in FinBERT id2label: {id2label}")
        index_order.append(matched[0])

    def teacher_probs_fn(texts):
        if isinstance(texts, str):
            texts_list = [texts]
        else:
            texts_list = list(texts)

        toks = finbert_tokenizer(
            texts_list,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt",
        ).to(device)

        with torch.no_grad():
            logits = finbert_model(**toks).logits
            probs = logits.softmax(dim=-1)  # [B, num_labels(finbert)]
            probs = probs[:, index_order]  # é‡æ–°æ’åºæˆ [neg, neu, pos]

        return probs

    print("âœ… [FinBERT] æœ¬åœ°ç¼“å­˜åŠ è½½æˆåŠŸã€‚")
    return teacher_probs_fn

def finbert_reward_base(
    prompts,
    model,
    tokenizer,
    label_order,
    teacher_probs_fn,
    text_list,
):
    """
    ç”¨ FinBERT teacher åšåˆ†å¸ƒå¯¹é½ï¼š
        reward = - [ KL(p_teacher || p_student) + KL(p_student || p_teacher) ]
    """
    if teacher_probs_fn is None:
        return [0.0] * len(prompts)

    with torch.no_grad():
        p_teacher = teacher_probs_fn(text_list)  # [B,3]
        p_student = get_student_probs_from_prompts(
            model, tokenizer, prompts, label_order
        )

        p_teacher = p_teacher.clamp_min(1e-12)
        p_student = p_student.clamp_min(1e-12)

        kl_ts = F.kl_div(
            p_teacher.log(), p_student, reduction="none"
        ).sum(dim=-1)
        kl_st = F.kl_div(
            p_student.log(), p_teacher, reduction="none"
        ).sum(dim=-1)

        sym_kl = kl_ts + kl_st
        rewards = (-sym_kl).detach().cpu().tolist()
    return rewards


def gt_logprob_reward_base(
    prompts,
    ground_truth,
    model,
    tokenizer,
    label_order,
):
    """
    Ground-truth log-prob reward:
        r_i = log p_theta(y_true | x_i)
    ç„¶ååœ¨ä¸€ä¸ª batch å†…åšæ ‡å‡†åŒ–ï¼ˆå‡å‡å€¼ / é™¤æ ‡å‡†å·®ï¼‰ã€‚

    prompts:      list[str]
    ground_truth: list[str] æˆ– list[int]ï¼Œæ¯”å¦‚ "negative" / 0
    """
    # 1) å…ˆç”¨ä½ å·²æœ‰çš„ helper ç®—å‡ºå¯¹ä¸‰ä¸ª label çš„åˆ†å¸ƒ
    with torch.no_grad():
        probs = get_student_probs_from_prompts(
            model, tokenizer, prompts, label_order
        )  # [B, 3]

    # 2) æ„å»º label -> index æ˜ å°„
    label2idx = {lab: i for i, lab in enumerate(label_order)}

    base_r = []
    for p_vec, y in zip(probs, ground_truth):
        lab = str(y).strip().lower()

        # ground_truth æ—¢å¯èƒ½æ˜¯ "negative"ï¼Œä¹Ÿå¯èƒ½æ˜¯ 0/1/2
        idx = None
        if lab.isdigit():
            idx_int = int(lab)
            if 0 <= idx_int < len(label_order):
                idx = idx_int
        else:
            idx = label2idx.get(lab, None)

        if idx is None:
            # æ‰¾ä¸åˆ°å°±ç»™ä¸ª 0ï¼Œå½“è¿™ä¸ªæ ·æœ¬æ²¡è´¡çŒ®
            base_r.append(0.0)
            continue

        # é˜²æ­¢ log(0)
        p = float(p_vec[idx].clamp(min=1e-12))
        base_r.append(math.log(p))

    if len(base_r) == 0:
        return [0.0] * len(prompts)

    # 3) åœ¨ batch å†…åšæ ‡å‡†åŒ–
    mean = float(np.mean(base_r))
    std = float(np.std(base_r))
    if std < 1e-8:
        std = 1.0  # é¿å…é™¤é›¶ï¼Œç­‰ä»·äºåªå‡å‡å€¼

    normed = [(r - mean) / std for r in base_r]
    return normed


############################################################
# 6. GRPO ä¸»å…¥å£ï¼šmain é‡Œç›´æ¥è°ƒç”¨è¿™ä¸ª
############################################################

def run_grpo_trl(
    data_path,
    sft_lora_path,
    base_model_path,
    cache_dir,
    output_dir="./outputs/grpo_output",
    perturb_data=True,
    use_finbert=True,
    finbert_model_name="ProsusAI/finbert",
    w_gt: float = 1.0,
    w_fin: float = 1.0,
    w_cons: float = 1.0,
    w_sft_kl: float = 0.1,
    resume: bool = False
):
    """
    data_path: CSV è·¯å¾„ï¼Œæ¯”å¦‚ "data/all-data.csv"
    sft_lora_path: ä½  SFT è®­ç»ƒä¿å­˜çš„ LoRA ç›®å½•
    base_model_path: LLAMA_MODEL_NAME
    cache_dir: CACHE_DIR
    output_dir: GRPO è¾“å‡ºç›®å½•
    perturb_data: æ˜¯å¦ä½¿ç”¨ clean+perturbed æˆå¯¹æ•°æ®
    use_finbert: æ˜¯å¦åœ¨å†…éƒ¨æ„å»º FinBERT teacher
    w_fin: FinBERT reward æƒé‡
    w_cons: ä¸€è‡´æ€§ reward æƒé‡
    """
    # 1) åŠ è½½å¹¶è½¬æ¢æ•°æ®
    print("ğŸ“¦ Loading & splitting raw data for GRPO ...")
    X_train_raw, X_test_raw, X_eval_raw = load_split_raw_data(
        data_path,
        perturb_data=perturb_data,
    )

    print("ğŸ”§ Converting data to HF dataset...")
    train_dataset = convert_to_hf_dataset(X_train_raw)

    # 2) åŠ è½½ tokenizerï¼ˆå¿…é¡»è·Ÿ SFT é˜¶æ®µä¸€è‡´ï¼‰
    print("ğŸ”§ Loading tokenizer from SFT checkpoint...")
    tokenizer = AutoTokenizer.from_pretrained(
        sft_lora_path,
        cache_dir=cache_dir,
        local_files_only=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    # 3) åŠ è½½ base LLaMA + SFT LoRA ï¼ˆæ”¹æˆ 4bit QLoRA é£æ ¼ï¼‰
    print("ğŸ”§ Loading base LLaMA model in 4bit (QLoRA style)...")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,  # âœ… ç»Ÿä¸€ç”¨ fp16
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        cache_dir=cache_dir,
        local_files_only=True,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,  # âœ… æ˜ç¡®å‘Šè¯‰ HF ç”¨ fp16
        device_map="auto",
    )

    # ä¿è¯ vocab size ä¸ SFT/GRPO ä½¿ç”¨çš„ tokenizer ä¸€è‡´ï¼ˆé¿å… 32000 vs 32002 é—®é¢˜ï¼‰
    if base_model.get_input_embeddings().num_embeddings != len(tokenizer):
        base_model.resize_token_embeddings(len(tokenizer))

    print("ğŸ”§ Loading SFT LoRA adapter...")
    model = PeftModel.from_pretrained(
        base_model,
        sft_lora_path,
        is_trainable=True,
    )

    # ================================
    # â­ å†»ç»“çš„ SFT teacherï¼Œç”¨äº KL æ­£åˆ™
    # ================================
    print("ğŸ”§ Loading frozen SFT teacher for KL regularization...")
    teacher_base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        cache_dir=cache_dir,
        local_files_only=True,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    if teacher_base_model.get_input_embeddings().num_embeddings != len(tokenizer):
        teacher_base_model.resize_token_embeddings(len(tokenizer))

    sft_teacher_model = PeftModel.from_pretrained(
        teacher_base_model,
        sft_lora_path,
        is_trainable=False,  # ä¸è®­ç»ƒ
    )
    sft_teacher_model.eval()
    for p in sft_teacher_model.parameters():
        p.requires_grad_(False)

    label_order = LABEL_ORDER
    teacher_probs_fn = None

    if teacher_probs_fn is None and use_finbert:
        print(f"ğŸ”§ Loading FinBERT teacher model: {finbert_model_name}")
        teacher_probs_fn = build_finbert_teacher(
            finbert_model_name=finbert_model_name,
            label_order=label_order,
        )

    # 5) GRPO é…ç½®
    # grpo_args = GRPOConfig(
    #     output_dir=output_dir,
    #     learning_rate=5e-6,
    #     per_device_train_batch_size=2,
    #     gradient_accumulation_steps=2,
    #     num_generations=4,
    #     max_prompt_length=512,
    #     max_completion_length=4,
    #     num_train_epochs=1,
    #     logging_steps=20,
    #     fp16=True,
    #     bf16=False,
    #     report_to="none",
    #     save_steps=200,     # æ¯ 200 step ä¿å­˜ä¸€æ¬¡ checkpoint
    #     save_total_limit=3  # åªä¿ç•™ 3 ä¸ª checkpoint
    # )
    grpo_args = GRPOConfig(
        output_dir=output_dir,
        learning_rate=5e-6,  # ä¿ç•™å°±è¡Œï¼Œåæ­£æ¢¯åº¦â‰ˆ0
        per_device_train_batch_size=2,  # â†“ è°ƒå°ï¼Œçœæ˜¾å­˜
        gradient_accumulation_steps=2,  # â†“ ä¸ç”¨ç´¯ç§¯äº†ï¼Œåæ­£åªæ˜¯å¯¹ç…§å®éªŒ
        num_generations=4,  # â†“ æ¯ä¸ª prompt åªé‡‡æ · 1 ä¸ª completionï¼Œå°±å¤Ÿäº†
        max_prompt_length=256,  # â†“ prompt æˆªæ–­çŸ­ä¸€ç‚¹ï¼ŒåŠ å¿« forward
        max_completion_length=4,  # ç»´æŒä¸€ä¸ªå¾ˆå°çš„ completion é•¿åº¦å³å¯
        num_train_epochs=1,  # åªè·‘ 1 ä¸ª epoch
        logging_steps=50,  # æ—¥å¿—ä¸ç”¨å¤ªé¢‘ç¹
        fp16=True,
        bf16=False,
        report_to="none",
        save_steps=10_000,  # è¿œå¤§äºæ€» step â†’ ä¸­é€”åŸºæœ¬ä¸ä¼š save
        save_total_limit=1  # åªä¿ç•™æœ€åä¸€ä¸ª checkpoint å°±è¡Œ
    )

    # 6) å®šä¹‰æœ€ç»ˆç”¨åˆ°çš„ reward ç»„åˆ

    import numpy as np
    def gt_logprob_reward(prompts, completions, ground_truth, **kwargs):
        """
        ä½¿ç”¨ log p(y_true | x) ä½œä¸º rewardï¼Œå†ä¹˜ä»¥ w_gtã€‚
        æ³¨æ„ï¼šä¸ä¾èµ– completionsï¼Œåªä¾èµ–å½“å‰ç­–ç•¥çš„åˆ†å¸ƒã€‚
        """
        base_r = gt_logprob_reward_base(
            prompts=prompts,
            ground_truth=ground_truth,
            model=model,
            tokenizer=tokenizer,
            label_order=label_order,
        )
        return [w_gt * r for r in base_r]

    def finbert_reward(prompts, completions, ground_truth, orig_text=None, **kwargs):
        if teacher_probs_fn is None or w_fin is None or w_fin <= 0:
            return [0.0] * len(prompts)
        if orig_text is None:
            orig_text = prompts  # fallback

        base_r = finbert_reward_base(
            prompts=prompts,
            model=model,
            tokenizer=tokenizer,
            label_order=label_order,
            teacher_probs_fn=teacher_probs_fn,
            text_list=orig_text,
        )  # list[float], é€šå¸¸æ˜¯è´Ÿçš„

        # â­ æ¯ä¸ª batch å†…åšä¸€æ¬¡æ ‡å‡†åŒ–ï¼šmean=0, std=1
        m = float(np.mean(base_r))
        s = float(np.std(base_r)) + 1e-8
        normed = [(r - m) / s for r in base_r]

        return [w_fin * r for r in normed]

    def consistency_reward(prompts, completions, ground_truth, pert_prompt=None, **kwargs):
        if not perturb_data or pert_prompt is None or w_cons is None or w_cons <= 0:
            return [0.0] * len(prompts)

        base_r = consistency_reward_base(
            prompts=prompts,
            model=model,
            tokenizer=tokenizer,
            label_order=label_order,
            pert_prompts_list=pert_prompt,
        )  # list[float], å¤šåŠä¹Ÿæ˜¯è´Ÿçš„

        # å¯èƒ½æœ‰ä¸€éƒ¨åˆ†å…¨ 0ï¼ˆæ²¡æœ‰æ‰°åŠ¨ï¼‰ï¼Œå¯ä»¥æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦å…¨ç›¸åŒ
        if len(set(base_r)) <= 1:
            # æ²¡æœ‰å·®å¼‚ï¼Œå°±ç®—äº†ï¼Œç›´æ¥å½“ 0 è´¡çŒ®
            return [0.0] * len(prompts)

        m = float(np.mean(base_r))
        s = float(np.std(base_r)) + 1e-8
        normed = [(r - m) / s for r in base_r]

        return [w_cons * r for r in normed]

        # ==========================================
        # â­ æ–°å¢ï¼šSFT KL æ­£åˆ™ rewardï¼š- KL(student || SFT_teacher)
        # ==========================================

    def sft_kl_reward(prompts, completions, ground_truth, **kwargs):
        """
        KL-to-SFT:
            reward = - w_sft_kl * KL( p_student || p_sft_teacher )
        ç›´è§‚ï¼šä¸å¸Œæœ›å½“å‰ç­–ç•¥åç¦» SFT è¿‡è¿œã€‚
        """
        if w_sft_kl is None or w_sft_kl <= 0.0:
            return [0.0] * len(prompts)

        with torch.no_grad():
            p_student = get_student_probs_from_prompts(
                model, tokenizer, prompts, label_order
            )
            p_teacher = get_student_probs_from_prompts(
                sft_teacher_model, tokenizer, prompts, label_order
            )

            p_student = p_student.clamp_min(1e-12)
            p_teacher = p_teacher.clamp_min(1e-12)

            # KL(p_student || p_teacher)
            kl_st = F.kl_div(
                p_student.log(), p_teacher, reduction="none"
            ).sum(dim=-1)  # [B]

            rewards = (-w_sft_kl * kl_st).detach().cpu().tolist()
        return rewards

    reward_funcs = [
        gt_logprob_reward
    ]

    # åªè¦ use_finbert ä¸” w_fin>0 ä¸” teacher çœŸçš„åŠ è½½æˆåŠŸï¼Œå°±å¯ç”¨ FinBERT reward
    if use_finbert and (w_fin is not None and w_fin > 0) and teacher_probs_fn is not None:
        reward_funcs.append(finbert_reward)

    # åªè¦æœ‰æ‰°åŠ¨æ•°æ®ä¸” w_cons>0ï¼Œå°±å¯ç”¨ä¸€è‡´æ€§ reward
    if perturb_data and (w_cons is not None and w_cons > 0):
        reward_funcs.append(consistency_reward)

    # â­ æ–°å¢ï¼šSFT KL æ­£åˆ™
    if w_sft_kl is not None and w_sft_kl > 0.0:
        reward_funcs.append(sft_kl_reward)

    # 7) æ„å»º GRPOTrainer
    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=reward_funcs,
        args=grpo_args,
        train_dataset=train_dataset,
    )

    print("ğŸ”¥ Starting GRPO training...")

    if resume:
        ckpts = glob.glob(os.path.join(output_dir, "checkpoint-*"))
        ckpts = sorted(ckpts, key=os.path.getmtime)
        if len(ckpts) > 0:
            last_ckpt = ckpts[-1]
            print(f"ğŸ” Found checkpoint: {last_ckpt}, resuming from it...")
            trainer.train(resume_from_checkpoint=last_ckpt)
        else:
            print("âš ï¸ Asked to resume but no checkpoint found, training from scratch...")
            trainer.train()
    else:
        print("ğŸ†• Forced fresh run (ignore checkpoints).")
        trainer.train()

    print("ğŸ’¾ Saving GRPO-tuned model...")
    trainer.save_model(output_dir)

    return trainer

