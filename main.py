import warnings

warnings.filterwarnings("ignore")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM
import torch

import argparse
from datasets import Dataset
from datetime import datetime

from configs import peft_config, train_args, CACHE_DIR, LLAMA_MODEL_NAME
from data_utils.dataset_build import load_and_split_data
from data_utils.evaluation import evaluate
from models.load_llama import load_llama
from models.predict_llama import predict
from training.sft_trainer import run_sft
from training.grpo_trainer import GRPOTrainer

from peft import PeftModel


def main():
    # ======== 1ï¸âƒ£ å‚æ•°è§£æ ========
    parser = argparse.ArgumentParser(description="Run sentiment classification pipeline")
    parser.add_argument("--run_sft", action="store_true", help="Run supervised fine-tuning (LoRA)")
    parser.add_argument("--run_grpo", action="store_true", help="Run GRPO fine-tuning (policy optimization)")
    args = parser.parse_args()

    # ======== 2ï¸âƒ£ åŠ è½½æ•°æ® ========
    X_train, X_test, X_eval, y_true, test = load_and_split_data("data/all-data.csv")
    train_data = Dataset.from_pandas(X_train)
    eval_data = Dataset.from_pandas(X_eval)
    # breakpoint()
    # ======== 3ï¸âƒ£ æ¨¡å¼é€‰æ‹© ========

    # === (1) SFT å¾®è°ƒ + é¢„æµ‹ + è¯„ä¼° ===
    if args.run_sft:
        print("\n================ SFT (LoRA) MODE ================\n")

        model, tok = load_llama(LLAMA_MODEL_NAME, CACHE_DIR)

        # breakpoint()
        # åŠ¨æ€ä¿å­˜è·¯å¾„
        time_tag = datetime.now().strftime("%Y%m%d")
        train_args.output_dir = f"./outputs/sft_{LLAMA_MODEL_NAME.split('/')[-1]}_{time_tag}"
        # print(f"ğŸ“ Model will be saved to: {train_args.output_dir}\n")

        # === 1ï¸âƒ£ è®­ç»ƒ ===
        run_sft(
            model=model,
            tokenizer=tok,
            train_data=train_data,
            eval_data=eval_data,
            training_args=train_args,
            peft_config=peft_config
        )
        print("âœ… SFT training finished!\n")

        # === 2ï¸âƒ£ åŠ è½½è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ ===
        print("â†’ Loading fine-tuned LoRA model for evaluation...")

        # model = AutoPeftModelForCausalLM.from_pretrained(
        #     train_args.output_dir,
        #     torch_dtype=torch.float16,
        #     device_map="auto"
        # )
        # merged_model = model.merge_and_unload()  # â† åˆå¹¶ LoRA æƒé‡

        # tokenizer = AutoTokenizer.from_pretrained(train_args.output_dir)
        # tokenizer.pad_token = tokenizer.eos_token
        # tokenizer.padding_side = "right"

        tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            cache_dir=CACHE_DIR,
            local_files_only=True,   # åªç”¨æœ¬åœ°ç¼“å­˜
            use_fast=True,
            trust_remote_code=True,
        )

        compute_dtype = getattr(torch, "float16")
        finetuned_model = train_args.output_dir

        model = AutoPeftModelForCausalLM.from_pretrained(
            finetuned_model,
            torch_dtype=compute_dtype,
            return_dict=True,
            low_cpu_mem_usage=True,
        )

        merged_model = model.merge_and_unload()
        merged_model.save_pretrained("./outputs/merged_model",safe_serialization=True, max_shard_size="2GB")
        tokenizer.save_pretrained("./outputs/merged_model")

        # === 3ï¸âƒ£ é¢„æµ‹ä¸è¯„ä¼° ===
        print("â†’ Generating predictions on test set...")
        preds = predict(X_test, merged_model, tokenizer)  # â† ç”¨ merged_model
        print("â†’ Evaluating...")
        evaluate(y_true, preds)

    # === (2) GRPO ä¼˜åŒ– === 
    elif args.run_grpo:
        print("\n================ GRPO MODE ================\n")
        grpo_trainer = GRPOTrainer(peft_config, train_args, CACHE_DIR, LLAMA_MODEL_NAME)
        grpo_trainer.train(X_train)
        print("\nâœ… GRPO fine-tuning done.\n")

    # === (3) Baseline é¢„æµ‹ === å®Œæˆ
    else:
        print("\n================ BASELINE MODE ================\n")
        model, tok = load_llama(LLAMA_MODEL_NAME, CACHE_DIR)
        preds = predict(X_test, model, tok)
        evaluate(y_true, preds)
        print("\nâœ… Baseline evaluation complete.\n")


if __name__ == "__main__":
    main()
