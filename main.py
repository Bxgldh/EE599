import warnings

warnings.filterwarnings("ignore")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM
import torch

import argparse
from datasets import Dataset
from datetime import datetime

from configs import peft_config, training_arguments, CACHE_DIR, LLAMA_MODEL_NAME
from data_utils.dataset_build import load_and_split_data
from data_utils.evaluation import evaluate
from models.load_llama import load_llama
from models.predict_llama import predict
from training.sft_trainer import run_sft
from training.grpo_trainer import GRPOTrainer


def main():
    # ======== 1ï¸âƒ£ å‚æ•°è§£æ ========
    parser = argparse.ArgumentParser(description="Run sentiment classification pipeline")
    parser.add_argument("--run_sft", action="store_true", help="Run supervised fine-tuning (LoRA)")
    parser.add_argument("--run_grpo", action="store_true", help="Run GRPO fine-tuning (policy optimization)")
    args = parser.parse_args()

    # ======== 2ï¸âƒ£ åŠ è½½æ•°æ® ========
    X_train, X_test, X_eval, y_true = load_and_split_data("data/all-data.csv")
    train_data = Dataset.from_pandas(X_train)
    eval_data = Dataset.from_pandas(X_eval)
    # breakpoint()
    # ======== 3ï¸âƒ£ æ¨¡å¼é€‰æ‹© ========

    # === (1) SFT å¾®è°ƒ + é¢„æµ‹ + è¯„ä¼° ===
    if args.run_sft:
        print("\n================ SFT (LoRA) MODE ================\n")

        model, tokenizer = load_llama(LLAMA_MODEL_NAME, CACHE_DIR)

        # åŠ¨æ€ä¿å­˜è·¯å¾„
        time_tag = datetime.now().strftime("%Y%m%d")
        training_arguments.output_dir = f"./outputs/sft_{LLAMA_MODEL_NAME.split('/')[-1]}_{time_tag}"
        # print(f"ğŸ“ Model will be saved to: {train_args.output_dir}\n")

        # === 1ï¸âƒ£ è®­ç»ƒ ===
        trainer = run_sft(
            model=model,
            tokenizer=tokenizer,
            train_data=train_data,
            eval_data=eval_data,
            training_args=training_arguments,
            peft_config=peft_config
        )
        trainer.train()
        trainer.save_model()
        tokenizer.save_pretrained(training_arguments.output_dir)
        print("âœ… SFT training finished!\n")

        # === 2ï¸âƒ£ åŠ è½½è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ ===
        print("â†’ Loading fine-tuned LoRA model for evaluation...")

        compute_dtype = getattr(torch, "float16")
        finetuned_model = training_arguments.output_dir
        print("å¾®è°ƒæ¨¡å‹ä½ç½®ï¼š" + finetuned_model)

        tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            cache_dir=CACHE_DIR,
            local_files_only=True,   # åªç”¨æœ¬åœ°ç¼“å­˜
            use_fast=True,
            trust_remote_code=True,
        )

        model = AutoPeftModelForCausalLM.from_pretrained(
            finetuned_model,
            torch_dtype=compute_dtype,
            return_dict=True,
            low_cpu_mem_usage=True,
            device_map="auto" # é‡è¦ï¼Œä¸åŠ ä¼šç”¨cpu
        )

        merged_model = model.merge_and_unload()
        merged_model.save_pretrained("./outputs/merged_model",safe_serialization=True, max_shard_size="2GB")
        tokenizer.save_pretrained("./outputs/merged_model")


        # === 3ï¸âƒ£ é¢„æµ‹ä¸è¯„ä¼° ===
        print("â†’ Generating predictions on test set...")
        preds = predict(X_test, merged_model, tokenizer)  # â† ç”¨ merged_model
        print("â†’ Evaluating...")
        evaluate(y_true, preds)

    # === (2) GRPO ä¼˜åŒ– === 
    elif args.run_grpo:
        print("\n================ GRPO MODE ================\n")
        grpo_trainer = GRPOTrainer(peft_config, training_arguments, CACHE_DIR, LLAMA_MODEL_NAME)
        grpo_trainer.train(X_train)
        print("\nâœ… GRPO fine-tuning done.\n")

    # === (3) Baseline é¢„æµ‹ === å®Œæˆ
    else:
        print("\n================ BASELINE MODE ================\n")
        model, tokenizer = load_llama(LLAMA_MODEL_NAME, CACHE_DIR)
        preds = predict(X_test, model, tokenizer)
        evaluate(y_true, preds)
        print("\nâœ… Baseline evaluation complete.\n")


if __name__ == "__main__":
    main()
