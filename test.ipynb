{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3d7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd3d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb13529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e7603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")\n",
    "\n",
    "\n",
    "def _to_model_device(toks, lm):\n",
    "    # 最靠谱：直接问模型“输入嵌入层”在哪个 device\n",
    "    dev = lm.get_input_embeddings().weight.device\n",
    "    return {k: v.to(dev) for k, v in toks.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda5647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3e9a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "567   Analyze the sentiment of the news headline enc...\n",
      "1752  Analyze the sentiment of the news headline enc...\n",
      "995   Analyze the sentiment of the news headline enc...\n",
      "601   Analyze the sentiment of the news headline enc...\n",
      "568   Analyze the sentiment of the news headline enc...\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/all-data.csv\"\n",
    "\n",
    "df = pd.read_csv(filename, \n",
    "                 names=[\"sentiment\", \"text\"],\n",
    "                 encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "X_train = list()\n",
    "X_test = list()\n",
    "\n",
    "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    train, test  = train_test_split(df[df.sentiment==sentiment], \n",
    "                                    train_size=300,\n",
    "                                    test_size=300, \n",
    "                                    random_state=42)\n",
    "    # print(test)\n",
    "    X_train.append(train)\n",
    "    X_test.append(test)\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "X_test = pd.concat(X_test)\n",
    "# print(X_test)\n",
    "\n",
    "eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n",
    "X_eval = df[df.index.isin(eval_idx)]\n",
    "X_eval = (X_eval\n",
    "          .groupby('sentiment', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "# 扰动用的\n",
    "import copy\n",
    "X_train_new = copy.deepcopy(X_train)\n",
    "X_test_new  = copy.deepcopy(X_test)\n",
    "X_eval_new  = copy.deepcopy(X_eval)\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "\n",
    "            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "\n",
    "            [{data_point[\"text\"]}] = \"\"\".strip()\n",
    "\n",
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"text\"])\n",
    "\n",
    "y_true = X_test.sentiment\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "print(X_test.head())\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25cd1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fee300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daea1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "cache_dir = \"/data3/zhenglon/huggingface/transformers\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,   # 强制只用你刚缓存到本地的文件\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a83b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 1, \n",
    "                        do_sample=False,\n",
    "                       )\n",
    "        result = pipe(prompt)\n",
    "        # print(result)\n",
    "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf09d6",
   "metadata": {},
   "source": [
    "# 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df09783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc6323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97335970",
   "metadata": {},
   "source": [
    "# Baseline 大部分都预测为了 label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc87e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 900/900 [00:00<00:00, 5505.67 examples/s]\n",
      "Map: 100%|██████████| 150/150 [00:00<00:00, 6391.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"trained_weigths\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,  \n",
    "        lora_dropout=0.1,\n",
    "        r=64, # rank 表示LoRA规模\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=3,                       # number of training epochs\n",
    "    per_device_train_batch_size=1,            # batch size per device during training\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,                         # log every 10 steps\n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
    "    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d15873",
   "metadata": {},
   "source": [
    "# LoRA 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f43746ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 20:53, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.698089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.515300</td>\n",
       "      <td>0.713891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=336, training_loss=0.7165863045624324, metrics={'train_runtime': 1258.2582, 'train_samples_per_second': 2.146, 'train_steps_per_second': 0.267, 'total_flos': 1.0717877554348032e+16, 'train_loss': 0.7165863045624324, 'epoch': 2.99})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f9db4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_weigths/tokenizer_config.json',\n",
       " 'trained_weigths/special_tokens_map.json',\n",
       " 'trained_weigths/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ce6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\n",
    "del [df, X_train, X_eval]\n",
    "del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cec6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ed2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./merged_model/tokenizer_config.json',\n",
       " './merged_model/special_tokens_map.json',\n",
       " './merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "finetuned_model = \"./trained_weigths/\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "cache_dir = \"/data3/zhenglon/huggingface/transformers\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,   # 只用本地缓存\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "     finetuned_model,\n",
    "     torch_dtype=compute_dtype,\n",
    "     return_dict=True,\n",
    "     low_cpu_mem_usage=True,\n",
    "     device_map=device,\n",
    ")\n",
    "print(f\"working on {device}\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b797b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 6,738,432,000 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 训练时最好有 pad_token_id\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 仅 LoRA 参与训练（保险起见再显式设一次）\n",
    "for n, p in model.named_parameters():\n",
    "    p.requires_grad = (\"lora_\" in n)\n",
    "\n",
    "# 可选：查看可训练参数量\n",
    "try:\n",
    "    model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total     = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64ea3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:27<00:00, 32.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.834\n",
      "Accuracy for label 0: 0.887\n",
      "Accuracy for label 1: 0.840\n",
      "Accuracy for label 2: 0.777\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       300\n",
      "           1       0.72      0.84      0.78       300\n",
      "           2       0.86      0.78      0.82       300\n",
      "\n",
      "    accuracy                           0.83       900\n",
      "   macro avg       0.84      0.83      0.84       900\n",
      "weighted avg       0.84      0.83      0.84       900\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[266  31   3]\n",
      " [ 13 252  35]\n",
      " [  1  66 233]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, merged_model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fd4d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'text': X_test[\"text\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbc7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy, re, random\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# fin_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "# fin_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_FIN = \"ProsusAI/finbert\"\n",
    "fin_tokenizer = AutoTokenizer.from_pretrained(_FIN)\n",
    "fin_model     = AutoModelForSequenceClassification.from_pretrained(_FIN).to(_DEVICE).eval()\n",
    "\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e07e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "SYN_DICT = {\n",
    "    \"rise\": [\"increase\",\"climb\",\"advance\"],\n",
    "    \"fall\": [\"decline\",\"drop\",\"slide\",\"dip\"],\n",
    "    \"strong\": [\"robust\",\"solid\",\"firm\"],\n",
    "    \"weak\": [\"soft\",\"fragile\",\"subdued\"]\n",
    "}\n",
    "\n",
    "def synonym_replace(text, max_repl=2):\n",
    "    doc = nlp(text)\n",
    "    repl_idx = [i for i,t in enumerate(doc) \n",
    "                if t.pos_ in {\"ADJ\",\"ADV\",\"NOUN\",\"VERB\"} and t.ent_type_==\"\" and t.lemma_.lower() in SYN_DICT]\n",
    "    random.shuffle(repl_idx)\n",
    "    out = list(token.text for token in doc)\n",
    "    for i in repl_idx[:max_repl]:\n",
    "        lemma = doc[i].lemma_.lower()\n",
    "        cand = random.choice(SYN_DICT[lemma])\n",
    "        out[i] = cand\n",
    "    return spacy.tokens.doc.Doc(doc.vocab, words=out).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a53e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "SYN_DICT = {\n",
    "    \"rise\": [\"increase\",\"climb\",\"advance\"],\n",
    "    \"fall\": [\"decline\",\"drop\",\"slide\",\"dip\"],\n",
    "    \"strong\": [\"robust\",\"solid\",\"firm\"],\n",
    "    \"weak\": [\"soft\",\"fragile\",\"subdued\"]\n",
    "}\n",
    "\n",
    "def synonym_replace(text, max_repl=2):\n",
    "    doc = nlp(text)\n",
    "    repl_idx = [i for i,t in enumerate(doc) \n",
    "                if t.pos_ in {\"ADJ\",\"ADV\",\"NOUN\",\"VERB\"} and t.ent_type_==\"\" and t.lemma_.lower() in SYN_DICT]\n",
    "    random.shuffle(repl_idx)\n",
    "    out = list(token.text for token in doc)\n",
    "    for i in repl_idx[:max_repl]:\n",
    "        lemma = doc[i].lemma_.lower()\n",
    "        cand = random.choice(SYN_DICT[lemma])\n",
    "        out[i] = cand\n",
    "    return spacy.tokens.doc.Doc(doc.vocab, words=out).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc8a1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#真实性过滤\n",
    "# 数据增强\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "def template_paraphrase_v1(\n",
    "    text: str,\n",
    "    *,\n",
    "    max_len_delta: float = 0.20,\n",
    "    keep_polarity_fn=None,  # 可传入一个函数，如 finbert_pred -> 返回 {\"label\": ...}\n",
    "    seed: int | None = 42\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    模板释义改写（v1）—— 为金融新闻标题设计的“轻度”改写器。\n",
    "    规则集：副词缓和、强度词中和、等价短语替换、轻度语态改写、名词化/动词化。\n",
    "    - 控制：改写后长度变化≤max_len_delta（默认20%）\n",
    "    - 可选：若提供 keep_polarity_fn( text )-> {\"label\": str }，则保证改写前后情感极性一致；不一致则回退原句。\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    orig = text\n",
    "    L0   = len(orig.split())\n",
    "\n",
    "    # ---------- 1) 等价短语替换（expectations / 指标描述） ----------\n",
    "    # 注意：先做短语级，再做词级，避免相互覆盖\n",
    "    rules_phrase = [\n",
    "        (r\"\\bbeat expectations\\b\",              \"exceeded expectations\"),\n",
    "        (r\"\\boutperformed expectations\\b\",      \"exceeded expectations\"),\n",
    "        (r\"\\bmissed expectations\\b\",            \"failed to meet expectations\"),\n",
    "        (r\"\\bfell short of expectations\\b\",     \"failed to meet expectations\"),\n",
    "        (r\"\\bmet expectations\\b\",               \"matched expectations\"),\n",
    "        (r\"\\babove guidance\\b\",                 \"above its guidance\"),\n",
    "        (r\"\\bbelow guidance\\b\",                 \"below its guidance\"),\n",
    "        (r\"\\bstrong demand\\b\",                  \"robust demand\"),\n",
    "        (r\"\\bweak demand\\b\",                    \"subdued demand\"),\n",
    "    ]\n",
    "\n",
    "    # ---------- 2) 强度副词中和（夸张→温和） ----------\n",
    "    rules_intensity = [\n",
    "        (r\"\\b(sharply|dramatically|significantly|substantially|considerably)\\b\", \"moderately\"),\n",
    "        (r\"\\b(soared|surged|skyrocketed)\\b\", \"jumped\"),\n",
    "        (r\"\\b(plunged|tumbled|cratered)\\b\",  \"fell\"),\n",
    "    ]\n",
    "\n",
    "    # ---------- 3) 动词缓和（加入副词/轻改结构） ----------\n",
    "    # 选择一个轻度副词，避免句子到处都是同一个词\n",
    "    soften_adv = random.choice([\"slightly\", \"modestly\", \"marginally\", \"somewhat\"])\n",
    "\n",
    "    def add_soft_adv(m):\n",
    "        # 如 rose -> rose slightly / increased -> increased modestly\n",
    "        return f\"{m.group(1)} {soften_adv}\"\n",
    "\n",
    "    rules_verbs_soften = [\n",
    "        # 上涨类\n",
    "        (r\"\\b(rose|increased|climbed|gained|advanced)\\b\", add_soft_adv),\n",
    "        # 下跌类\n",
    "        (r\"\\b(fell|declined|dropped|slid|weakened)\\b\",     add_soft_adv),\n",
    "    ]\n",
    "\n",
    "    # ---------- 4) 轻度语态/结构改写 ----------\n",
    "    rules_syntax = [\n",
    "        (r\"\\bThe company reported\\b\", \"A report from the company indicated\"),\n",
    "        (r\"\\bThe firm reported\\b\",    \"A report from the firm indicated\"),\n",
    "        (r\"\\brevenues? (rose|increased)\\b\", r\"revenue \\1\"),\n",
    "        (r\"\\bprofits? (fell|declined)\\b\",  r\"profit \\1\"),\n",
    "        (r\"\\bwas up\\b\", \"rose\"),\n",
    "        (r\"\\bwas down\\b\", \"declined\"),\n",
    "    ]\n",
    "\n",
    "    # ---------- 5) 依次应用规则 ----------\n",
    "    new = text\n",
    "\n",
    "    for pat, rep in rules_phrase:\n",
    "        new = re.sub(pat, rep, new, flags=re.I)\n",
    "\n",
    "    for pat, rep in rules_intensity:\n",
    "        new = re.sub(pat, rep, new, flags=re.I)\n",
    "\n",
    "    for pat, rep in rules_verbs_soften:\n",
    "        new = re.sub(pat, rep, new, flags=re.I)\n",
    "\n",
    "    for pat, rep in rules_syntax:\n",
    "        new = re.sub(pat, rep, new, flags=re.I)\n",
    "\n",
    "    # ---------- 6) 长度变化控制 ----------\n",
    "    L1 = len(new.split())\n",
    "    if abs(L1 - L0) / max(1, L0) > max_len_delta:\n",
    "        # 超过约束就回退到原句\n",
    "        new = orig\n",
    "\n",
    "    # ---------- 7) 可选：极性一致性检查（如传入 finbert_pred） ----------\n",
    "    if callable(keep_polarity_fn):\n",
    "        try:\n",
    "            if keep_polarity_fn(orig)[\"label\"] != keep_polarity_fn(new)[\"label\"]:\n",
    "                new = orig  # 极性变了，回退\n",
    "        except Exception:\n",
    "            # 避免评估器异常导致崩溃\n",
    "            new = orig\n",
    "\n",
    "    return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35eef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo 实体替换\n",
    "# SECTOR_PEERS = {\"Apple\":\"Microsoft\", \"JPMorgan\":\"Bank of America\", \"Exxon\":\"Chevron\"}\n",
    "SECTOR_PEERS ={\n",
    "    \"Barclays\": \"Citigroup\",\n",
    "    \"Citigroup\": \"Goldman Sachs\",\n",
    "    \"Goldman Sachs\": \"Deutsche Bank\",\n",
    "    \"Deutsche Bank\": \"Credit Suisse\",\n",
    "    \"Credit Suisse\": \"JPMorgan\",\n",
    "    \"JPMorgan\": \"UBS\",\n",
    "    \"UBS\": \"Bank of America\",\n",
    "    \"Bank of America\": \"Morgan Stanley\",\n",
    "    \"Morgan Stanley\": \"Barclays\",\n",
    "    \"United\": \"American Airlines\",\n",
    "    \"American Airlines\": \"British Airways\",\n",
    "    \"British Airways\": \"Delta\",\n",
    "    \"Delta\": \"Emirates\",\n",
    "    \"Emirates\": \"United\",\n",
    "    \"Orange\": \"China Mobile\",\n",
    "    \"China Mobile\": \"Verizon\",\n",
    "    \"Verizon\": \"Vodafone\",\n",
    "    \"Vodafone\": \"AT&T\",\n",
    "    \"AT&T\": \"Orange\",\n",
    "    \"Target\": \"CVS\",\n",
    "    \"CVS\": \"Home Depot\",\n",
    "    \"Home Depot\": \"Target\",\n",
    "    \"Microsoft\": \"Google\",\n",
    "    \"Google\": \"Apple\",\n",
    "    \"Apple\": \"Intel\",\n",
    "    \"Intel\": \"IBM\",\n",
    "    \"IBM\": \"Qualcomm\",\n",
    "    \"Qualcomm\": \"Microsoft\",\n",
    "    \"BP\": \"Shell\",\n",
    "    \"Shell\": \"BP\",\n",
    "    \"Volkswagen\": \"Ford\",\n",
    "    \"Ford\": \"Honda\",\n",
    "    \"Honda\": \"Volkswagen\",\n",
    "    \"Sony\": \"Fox\",\n",
    "    \"Fox\": \"Sony\",\n",
    "    \"X\": \"Snap\"\n",
    "}\n",
    "\n",
    "def perturb_numbers_entities(text):\n",
    "    def tweak_num(m):\n",
    "        x = float(m.group(1))\n",
    "        y = round(x + random.choice([-0.2, -0.1, 0.1, 0.2]), 1)  # 百分点微调\n",
    "        return f\"{y}%\"\n",
    "    text2 = re.sub(r\"(\\d+\\.?\\d*)\\s?%\", tweak_num, text)\n",
    "\n",
    "    for a,b in SECTOR_PEERS.items():\n",
    "        if a in text2:\n",
    "            text2 = text2.replace(a,b,1); break\n",
    "    return text2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a160205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_NER = {\"ORG\",\"GPE\",\"PRODUCT\",\"MONEY\",\"PERCENT\"}\n",
    "\n",
    "def entity_label_set(text: str) -> set:\n",
    "    doc = nlp(text)\n",
    "    # 若管线里没有 NER，返回空集（相当于不触发该约束）\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        return set()\n",
    "    return {ent.label_ for ent in doc.ents if ent.label_ in ALLOWED_NER}\n",
    "\n",
    "def pass_filter(src: str, tgt: str, sim_th: float = 0.85, len_ratio: float = 0.2):\n",
    "    # 语义相似度（SBERT）\n",
    "    emb = sbert.encode([src, tgt], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sim = float(util.cos_sim(emb[0], emb[1]))\n",
    "    if sim < sim_th:\n",
    "        return False, {\"sim\": sim}\n",
    "\n",
    "    # 长度变化约束\n",
    "    L0, L1 = len(src.split()), len(tgt.split())\n",
    "    if abs(L1 - L0) / max(1, L0) > len_ratio:\n",
    "        return False, {\"sim\": sim, \"len_ok\": False}\n",
    "\n",
    "    # 实体“类型集合”一致（允许同行替换）\n",
    "    src_labels = entity_label_set(src)\n",
    "    tgt_labels = entity_label_set(tgt)\n",
    "    if src_labels != tgt_labels:\n",
    "        return False, {\"sim\": sim, \"ner_ok\": False, \"src_ner\": sorted(src_labels), \"tgt_ner\": sorted(tgt_labels)}\n",
    "\n",
    "    return True, {\"sim\": sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73bb7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否对抗\n",
    "# def finbert_pred(text):\n",
    "#     scores = fin_pipe(text)[0]  # list of dicts: {'label':'positive','score':...}\n",
    "#     scores = {d['label'].lower(): d['score'] for d in scores}\n",
    "#     label = max(scores, key=scores.get)\n",
    "#     margin = scores[label] - max([v for k,v in scores.items() if k!=label])\n",
    "#     return label, margin, scores\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_pred(text: str):\n",
    "    enc = fin_tokenizer(text, return_tensors=\"pt\", truncation=True).to(_DEVICE)\n",
    "    logits = fin_model(**enc).logits               # [1, 3]\n",
    "    probs  = F.softmax(logits, dim=-1).squeeze(0)  # torch.Tensor，不用 numpy\n",
    "    id2label = {i: fin_model.config.id2label[i].lower() for i in range(probs.numel())}\n",
    "    scores   = {id2label[i]: float(probs[i].item()) for i in range(probs.numel())}\n",
    "    label    = max(scores, key=scores.get)\n",
    "    top = scores[label]; second = max(v for k,v in scores.items() if k != label)\n",
    "    margin = top - second\n",
    "    return label, margin, scores\n",
    "\n",
    "def adversarial_tag(orig, pert, margin_drop=0.2):\n",
    "    y0, m0, s0 = finbert_pred(orig)\n",
    "    y1, m1, s1 = finbert_pred(pert)\n",
    "    flipped = (y0 != y1)\n",
    "    weakened = (m0 - m1) >= margin_drop\n",
    "    return {\"orig\":y0,\"pert\":y1,\"flipped\":flipped,\"weakened\":weakened,\"m0\":m0,\"m1\":m1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e087642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9077935703098774,\n",
       "  'm1': 0.9077935703098774},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9077935703098774,\n",
       "  'm1': 0.9077935703098774},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9077935703098774,\n",
       "  'm1': 0.9077935703098774},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9347926788032055,\n",
       "  'm1': 0.9347926788032055},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9347926788032055,\n",
       "  'm1': 0.9342988766729832},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9347926788032055,\n",
       "  'm1': 0.9347926788032055},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9484887830913067,\n",
       "  'm1': 0.9484887830913067},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9484887830913067,\n",
       "  'm1': 0.9484887830913067},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9484887830913067,\n",
       "  'm1': 0.9501754716038704},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7377919554710388,\n",
       "  'm1': 0.7377919554710388},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7377919554710388,\n",
       "  'm1': 0.7377919554710388},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7377919554710388,\n",
       "  'm1': 0.7377919554710388},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8294460028409958,\n",
       "  'm1': 0.8294460028409958},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8294460028409958,\n",
       "  'm1': 0.8294460028409958},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8294460028409958,\n",
       "  'm1': 0.8294460028409958},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9545999467372894,\n",
       "  'm1': 0.9545999467372894},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9545999467372894,\n",
       "  'm1': 0.9555051103234291},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9545999467372894,\n",
       "  'm1': 0.9545999467372894},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7990042194724083,\n",
       "  'm1': 0.7990042194724083},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7990042194724083,\n",
       "  'm1': 0.7990042194724083},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7990042194724083,\n",
       "  'm1': 0.7990042194724083},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5980176031589508,\n",
       "  'm1': 0.5980176031589508},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5980176031589508,\n",
       "  'm1': 0.5980176031589508},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5980176031589508,\n",
       "  'm1': 0.5980176031589508},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.834733746945858,\n",
       "  'm1': 0.834733746945858},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.834733746945858,\n",
       "  'm1': 0.834733746945858},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.834733746945858,\n",
       "  'm1': 0.834733746945858},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8269194141030312,\n",
       "  'm1': 0.8269194141030312},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8269194141030312,\n",
       "  'm1': 0.8269194141030312},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8269194141030312,\n",
       "  'm1': 0.8269194141030312},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7287526875734329,\n",
       "  'm1': 0.7287526875734329},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7287526875734329,\n",
       "  'm1': 0.7287526875734329},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7287526875734329,\n",
       "  'm1': 0.7287526875734329},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5977266281843185,\n",
       "  'm1': 0.5977266281843185},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5977266281843185,\n",
       "  'm1': 0.5977266281843185},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5977266281843185,\n",
       "  'm1': 0.5977266281843185},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9489133190363646,\n",
       "  'm1': 0.9489133190363646},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9489133190363646,\n",
       "  'm1': 0.9489133190363646},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9489133190363646,\n",
       "  'm1': 0.9489133190363646},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9410907737910748,\n",
       "  'm1': 0.9410907737910748},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9410907737910748,\n",
       "  'm1': 0.9410907737910748},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9410907737910748,\n",
       "  'm1': 0.9410907737910748},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7542833760380745,\n",
       "  'm1': 0.7542833760380745},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7542833760380745,\n",
       "  'm1': 0.7542833760380745},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7542833760380745,\n",
       "  'm1': 0.7542833760380745},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8919427916407585,\n",
       "  'm1': 0.8919427916407585},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8919427916407585,\n",
       "  'm1': 0.8919427916407585},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8919427916407585,\n",
       "  'm1': 0.8919427916407585},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.48969465494155884,\n",
       "  'm1': 0.48969465494155884},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.48969465494155884,\n",
       "  'm1': 0.48969465494155884},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.48969465494155884,\n",
       "  'm1': 0.48969465494155884},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9245155677199364,\n",
       "  'm1': 0.9245155677199364},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9245155677199364,\n",
       "  'm1': 0.9258041922003031},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9245155677199364,\n",
       "  'm1': 0.9245155677199364},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9487853683531284,\n",
       "  'm1': 0.9487853683531284},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9487853683531284,\n",
       "  'm1': 0.9532344751060009},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9487853683531284,\n",
       "  'm1': 0.9487853683531284},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.902728695422411,\n",
       "  'm1': 0.902728695422411},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.902728695422411,\n",
       "  'm1': 0.902728695422411},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.902728695422411,\n",
       "  'm1': 0.9068156145513058},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9623177256435156,\n",
       "  'm1': 0.9623177256435156},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9623177256435156,\n",
       "  'm1': 0.9623177256435156},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9623177256435156,\n",
       "  'm1': 0.9623177256435156},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9307886064052582,\n",
       "  'm1': 0.9307886064052582},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9307886064052582,\n",
       "  'm1': 0.9307886064052582},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9307886064052582,\n",
       "  'm1': 0.9307886064052582},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.6059796363115311,\n",
       "  'm1': 0.6059796363115311},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.6059796363115311,\n",
       "  'm1': 0.6059796363115311},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.6059796363115311,\n",
       "  'm1': 0.6059796363115311},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9222335144877434,\n",
       "  'm1': 0.9222335144877434},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9222335144877434,\n",
       "  'm1': 0.9222335144877434},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9222335144877434,\n",
       "  'm1': 0.9222335144877434},\n",
       " {'orig': 'Name of Applicant : Jot Automation OYName of Inventor : Mammila Tuomo , Piirainen Mika and Kellokoski MikaApplication No. : 2424-KOLNP-2008 ADate of filing of Application : 16-06-2008Publication Date : 30/01/2009',\n",
       "  'pert': 'Name of Applicant : Jot Automation OYName of Inventor : Mammila Tuomo , Piirainen Mika and Kellokoski MikaApplication No . : 2424 - KOLNP-2008 ADate of filing of Application : 16 - 06 - 2008Publication Date : 30/01/2009 ',\n",
       "  'passed': False,\n",
       "  'sim': 1.0,\n",
       "  'len_ok': False},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9165843203663826,\n",
       "  'm1': 0.9165843203663826},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9165843203663826,\n",
       "  'm1': 0.9165843203663826},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9422990567982197,\n",
       "  'm1': 0.9422990567982197},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9422990567982197,\n",
       "  'm1': 0.9422990567982197},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9422990567982197,\n",
       "  'm1': 0.9422990567982197},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9358501471579075,\n",
       "  'm1': 0.9358501471579075},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9358501471579075,\n",
       "  'm1': 0.9358501471579075},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9358501471579075,\n",
       "  'm1': 0.9358501471579075},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.4792754054069519,\n",
       "  'm1': 0.4792754054069519},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.4792754054069519,\n",
       "  'm1': 0.4792754054069519},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.4792754054069519,\n",
       "  'm1': 0.4792754054069519},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9137657471001148,\n",
       "  'm1': 0.9137657471001148},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9137657471001148,\n",
       "  'm1': 0.9137657471001148},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9137657471001148,\n",
       "  'm1': 0.9137657471001148},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8975424692034721,\n",
       "  'm1': 0.8975424692034721},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8975424692034721,\n",
       "  'm1': 0.8975424692034721},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8975424692034721,\n",
       "  'm1': 0.8970519080758095},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.18550005555152893,\n",
       "  'm1': 0.18550005555152893},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.18550005555152893,\n",
       "  'm1': 0.18550005555152893},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.18550005555152893,\n",
       "  'm1': 0.18550005555152893},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8087137043476105,\n",
       "  'm1': 0.8087137043476105},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8087137043476105,\n",
       "  'm1': 0.8087137043476105},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8087137043476105,\n",
       "  'm1': 0.8087137043476105},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.885414469987154,\n",
       "  'm1': 0.885414469987154},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.885414469987154,\n",
       "  'm1': 0.885414469987154},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.885414469987154,\n",
       "  'm1': 0.885414469987154},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8958318121731281,\n",
       "  'm1': 0.8958318121731281},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8958318121731281,\n",
       "  'm1': 0.8958318121731281},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8958318121731281,\n",
       "  'm1': 0.8958318121731281},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.6648699939250946,\n",
       "  'm1': 0.6648699939250946},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.6648699939250946,\n",
       "  'm1': 0.6648699939250946},\n",
       " {'orig': 'VDW combined with LXE devices enhances productivity , enabling workers to use a single device to perform voice , scanning and keyboard functions .',\n",
       "  'pert': 'VDW combined with LSnapE devices enhances productivity , enabling workers to use a single device to perform voice , scanning and keyboard functions .',\n",
       "  'passed': False,\n",
       "  'sim': 0.8497050404548645},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7330998331308365,\n",
       "  'm1': 0.7330998331308365},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7330998331308365,\n",
       "  'm1': 0.7330998331308365},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7330998331308365,\n",
       "  'm1': 0.7330998331308365},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8919891119003296,\n",
       "  'm1': 0.8919891119003296},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8919891119003296,\n",
       "  'm1': 0.8919891119003296},\n",
       " {'orig': 'SysOpen Digia Plc , Press release , 7 February 2006 IBM Finland has rewarded its most distinguished partner companies for 2005 .',\n",
       "  'pert': 'SysOpen Digia Plc , Press release , 7 February 2006 Qualcomm Finland has rewarded its most distinguished partner companies for 2005 .',\n",
       "  'passed': False,\n",
       "  'sim': 0.8680753111839294,\n",
       "  'ner_ok': False,\n",
       "  'src_ner': ['GPE', 'ORG'],\n",
       "  'tgt_ner': ['ORG']},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9052532576024532,\n",
       "  'm1': 0.9052532576024532},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9052532576024532,\n",
       "  'm1': 0.9052532576024532},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9052532576024532,\n",
       "  'm1': 0.9052532576024532},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9202062971889973,\n",
       "  'm1': 0.9202062971889973},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9202062971889973,\n",
       "  'm1': 0.9202062971889973},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9202062971889973,\n",
       "  'm1': 0.9202062971889973},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8265360295772552,\n",
       "  'm1': 0.8265360295772552},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8265360295772552,\n",
       "  'm1': 0.8265360295772552},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8265360295772552,\n",
       "  'm1': 0.8265360295772552},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.913769543170929,\n",
       "  'm1': 0.913769543170929},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.913769543170929,\n",
       "  'm1': 0.913769543170929},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.913769543170929,\n",
       "  'm1': 0.913769543170929},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.411124587059021,\n",
       "  'm1': 0.411124587059021},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.411124587059021,\n",
       "  'm1': 0.411124587059021},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.411124587059021,\n",
       "  'm1': 0.411124587059021},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9010761827230453,\n",
       "  'm1': 0.9010761827230453},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9010761827230453,\n",
       "  'm1': 0.9010761827230453},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9010761827230453,\n",
       "  'm1': 0.9010761827230453},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.019508540630340576,\n",
       "  'm1': 0.019508540630340576},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.019508540630340576,\n",
       "  'm1': 0.019508540630340576},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.019508540630340576,\n",
       "  'm1': 0.019508540630340576},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.906914371997118,\n",
       "  'm1': 0.906914371997118},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.906914371997118,\n",
       "  'm1': 0.905904907733202},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.906914371997118,\n",
       "  'm1': 0.906914371997118},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.760931208729744,\n",
       "  'm1': 0.760931208729744},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.760931208729744,\n",
       "  'm1': 0.760931208729744},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.760931208729744,\n",
       "  'm1': 0.760931208729744},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7904538065195084,\n",
       "  'm1': 0.7904538065195084},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7904538065195084,\n",
       "  'm1': 0.7904538065195084},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7904538065195084,\n",
       "  'm1': 0.7904538065195084},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8582336455583572,\n",
       "  'm1': 0.8582336455583572},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8582336455583572,\n",
       "  'm1': 0.8582336455583572},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8582336455583572,\n",
       "  'm1': 0.8582336455583572},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9612599536776543,\n",
       "  'm1': 0.9612599536776543},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9612599536776543,\n",
       "  'm1': 0.9612599536776543},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9612599536776543,\n",
       "  'm1': 0.9612718736752868},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8038421422243118,\n",
       "  'm1': 0.8038421422243118},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8038421422243118,\n",
       "  'm1': 0.8038421422243118},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8038421422243118,\n",
       "  'm1': 0.8038421422243118},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9243750907480717,\n",
       "  'm1': 0.9240607935935259},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9243750907480717,\n",
       "  'm1': 0.9259266387671232},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9243750907480717,\n",
       "  'm1': 0.9243750907480717},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9330745507031679,\n",
       "  'm1': 0.9330745507031679},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9330745507031679,\n",
       "  'm1': 0.9330745507031679},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9330745507031679,\n",
       "  'm1': 0.9330745507031679},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.03268668055534363,\n",
       "  'm1': 0.03268668055534363},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.03268668055534363,\n",
       "  'm1': 0.03268668055534363},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.03268668055534363,\n",
       "  'm1': 0.03268668055534363},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9577674586325884,\n",
       "  'm1': 0.9577674586325884},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9577674586325884,\n",
       "  'm1': 0.9561444241553545},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9577674586325884,\n",
       "  'm1': 0.9577674586325884},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9254800230264664,\n",
       "  'm1': 0.9254800230264664},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9254800230264664,\n",
       "  'm1': 0.9533900078386068},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9254800230264664,\n",
       "  'm1': 0.9254800230264664},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9124080948531628,\n",
       "  'm1': 0.9124080948531628},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9124080948531628,\n",
       "  'm1': 0.9124080948531628},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9124080948531628,\n",
       "  'm1': 0.9124080948531628},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8222218155860901,\n",
       "  'm1': 0.8222218155860901},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8222218155860901,\n",
       "  'm1': 0.8222218155860901},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8222218155860901,\n",
       "  'm1': 0.8222218155860901},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9217464979737997,\n",
       "  'm1': 0.9217464979737997},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9217464979737997,\n",
       "  'm1': 0.9217464979737997},\n",
       " {'orig': 'Nordea Bank AB publ holds 6.000 Alma Media shares , representing 0.008 % of share capital and voting rights .',\n",
       "  'pert': 'Nordea Bank AB publ holds 6.000 Alma Media shares , representing -0.2% of share capital and voting rights .',\n",
       "  'passed': False,\n",
       "  'sim': 0.9985580444335938,\n",
       "  'ner_ok': False,\n",
       "  'src_ner': ['ORG', 'PERCENT'],\n",
       "  'tgt_ner': ['ORG']},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5584023296833038,\n",
       "  'm1': 0.5584023296833038},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5584023296833038,\n",
       "  'm1': 0.5584023296833038},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5584023296833038,\n",
       "  'm1': 0.5584023296833038},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.613151803612709,\n",
       "  'm1': 0.613151803612709},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.613151803612709,\n",
       "  'm1': 0.613151803612709},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.613151803612709,\n",
       "  'm1': 0.613151803612709},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9598067291080952,\n",
       "  'm1': 0.9598067291080952},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9598067291080952,\n",
       "  'm1': 0.9598067291080952},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9598067291080952,\n",
       "  'm1': 0.9598067291080952},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8896437101066113,\n",
       "  'm1': 0.8896437101066113},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8896437101066113,\n",
       "  'm1': 0.8896437101066113},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8896437101066113,\n",
       "  'm1': 0.8896437101066113},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': True,\n",
       "  'weakened': True,\n",
       "  'm0': 0.7988985329866409,\n",
       "  'm1': 0.049707382917404175},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7988985329866409,\n",
       "  'm1': 0.7817716598510742},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7988985329866409,\n",
       "  'm1': 0.7988985329866409},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.858126312494278,\n",
       "  'm1': 0.858126312494278},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.858126312494278,\n",
       "  'm1': 0.858126312494278},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.858126312494278,\n",
       "  'm1': 0.858126312494278},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': True,\n",
       "  'm0': 0.8390576541423798,\n",
       "  'm1': 0.5000439286231995},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8390576541423798,\n",
       "  'm1': 0.9116283729672432},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8390576541423798,\n",
       "  'm1': 0.8390576541423798},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9601219054311514,\n",
       "  'm1': 0.9601219054311514},\n",
       " {'orig': \"TietoEnator was down 1.13 pct to 18.38 , extending recent lows after last week 's second-quarter report , dealers said .\",\n",
       "  'pert': \"TietoEnator declined 1.13 pct to 18.38 , extending recent lows after last week 's second-quarter report , dealers said .\",\n",
       "  'passed': False,\n",
       "  'sim': 0.8673475384712219,\n",
       "  'ner_ok': False,\n",
       "  'src_ner': [],\n",
       "  'tgt_ner': ['ORG']},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9601219054311514,\n",
       "  'm1': 0.9601219054311514},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9352008197456598,\n",
       "  'm1': 0.9352008197456598},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9352008197456598,\n",
       "  'm1': 0.9352008197456598},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9352008197456598,\n",
       "  'm1': 0.9352008197456598},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9543512873351574,\n",
       "  'm1': 0.9543512873351574},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9543512873351574,\n",
       "  'm1': 0.9558153711259365},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9543512873351574,\n",
       "  'm1': 0.9543512873351574},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8878322243690491,\n",
       "  'm1': 0.8878322243690491},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8878322243690491,\n",
       "  'm1': 0.8878322243690491},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8878322243690491,\n",
       "  'm1': 0.8878322243690491},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8858409225940704,\n",
       "  'm1': 0.8858409225940704},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8858409225940704,\n",
       "  'm1': 0.8858409225940704},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8858409225940704,\n",
       "  'm1': 0.8858409225940704},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9027462974190712,\n",
       "  'm1': 0.9027462974190712},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9027462974190712,\n",
       "  'm1': 0.9027462974190712},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9027462974190712,\n",
       "  'm1': 0.9027462974190712},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.762748509645462,\n",
       "  'm1': 0.762748509645462},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.762748509645462,\n",
       "  'm1': 0.762748509645462},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.762748509645462,\n",
       "  'm1': 0.762748509645462},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5581193566322327,\n",
       "  'm1': 0.5581193566322327},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5581193566322327,\n",
       "  'm1': 0.5581193566322327},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5581193566322327,\n",
       "  'm1': 0.5581193566322327},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5835045725107193,\n",
       "  'm1': 0.5835045725107193},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5835045725107193,\n",
       "  'm1': 0.5835045725107193},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5835045725107193,\n",
       "  'm1': 0.5835045725107193},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8650988601148129,\n",
       "  'm1': 0.8650988601148129},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8650988601148129,\n",
       "  'm1': 0.8650988601148129},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8650988601148129,\n",
       "  'm1': 0.8650988601148129},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5026034563779831,\n",
       "  'm1': 0.5026034563779831},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5026034563779831,\n",
       "  'm1': 0.5026034563779831},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5026034563779831,\n",
       "  'm1': 0.5026034563779831},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9266909025609493,\n",
       "  'm1': 0.9266909025609493},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9266909025609493,\n",
       "  'm1': 0.9266909025609493},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9266909025609493,\n",
       "  'm1': 0.9266909025609493},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9247646164149046,\n",
       "  'm1': 0.9247646164149046},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9247646164149046,\n",
       "  'm1': 0.9247646164149046},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9247646164149046,\n",
       "  'm1': 0.9247646164149046},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7883571684360504,\n",
       "  'm1': 0.7883571684360504},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7883571684360504,\n",
       "  'm1': 0.7883571684360504},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7883571684360504,\n",
       "  'm1': 0.7883571684360504},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7862311378121376,\n",
       "  'm1': 0.7862311378121376},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7862311378121376,\n",
       "  'm1': 0.7862311378121376},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7862311378121376,\n",
       "  'm1': 0.7862311378121376},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9424094129353762,\n",
       "  'm1': 0.9424094129353762},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9424094129353762,\n",
       "  'm1': 0.9424094129353762},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9424094129353762,\n",
       "  'm1': 0.9424094129353762},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9282899349927902,\n",
       "  'm1': 0.9282899349927902},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9282899349927902,\n",
       "  'm1': 0.9282899349927902},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9282899349927902,\n",
       "  'm1': 0.9282899349927902},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9232276696711779,\n",
       "  'm1': 0.9224106762558222},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9232276696711779,\n",
       "  'm1': 0.9247781783342361},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9232276696711779,\n",
       "  'm1': 0.9232276696711779},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5702942758798599,\n",
       "  'm1': 0.5702942758798599},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5702942758798599,\n",
       "  'm1': 0.5702942758798599},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5702942758798599,\n",
       "  'm1': 0.5702942758798599},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9053059704601765,\n",
       "  'm1': 0.9053059704601765},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9053059704601765,\n",
       "  'm1': 0.9053059704601765},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9053059704601765,\n",
       "  'm1': 0.9053059704601765},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8778064660727978,\n",
       "  'm1': 0.8778064660727978},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8778064660727978,\n",
       "  'm1': 0.8778064660727978},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8778064660727978,\n",
       "  'm1': 0.8778064660727978},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9617618098855019,\n",
       "  'm1': 0.9617618098855019},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9617618098855019,\n",
       "  'm1': 0.9617618098855019},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9617618098855019,\n",
       "  'm1': 0.9617618098855019},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5636214911937714,\n",
       "  'm1': 0.5636214911937714},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5636214911937714,\n",
       "  'm1': 0.5636214911937714},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.5636214911937714,\n",
       "  'm1': 0.5636214911937714},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9574229381978512,\n",
       "  'm1': 0.9574229381978512},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9574229381978512,\n",
       "  'm1': 0.9526749663054943},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9574229381978512,\n",
       "  'm1': 0.9574229381978512},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7070772349834442,\n",
       "  'm1': 0.7070772349834442},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7070772349834442,\n",
       "  'm1': 0.7070772349834442},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7070772349834442,\n",
       "  'm1': 0.7070772349834442},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8044560924172401,\n",
       "  'm1': 0.8044560924172401},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8044560924172401,\n",
       "  'm1': 0.8044560924172401},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8044560924172401,\n",
       "  'm1': 0.8044560924172401},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8778195753693581,\n",
       "  'm1': 0.8778195753693581},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8778195753693581,\n",
       "  'm1': 0.8778195753693581},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.8778195753693581,\n",
       "  'm1': 0.8778195753693581},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9602297637611628,\n",
       "  'm1': 0.9602297637611628},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9602297637611628,\n",
       "  'm1': 0.9602297637611628},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9602297637611628,\n",
       "  'm1': 0.9602297637611628},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.879854541271925,\n",
       "  'm1': 0.873505175113678},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.879854541271925,\n",
       "  'm1': 0.879854541271925},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.879854541271925,\n",
       "  'm1': 0.879854541271925},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9068921767175198,\n",
       "  'm1': 0.9068921767175198},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9068921767175198,\n",
       "  'm1': 0.9068921767175198},\n",
       " {'orig': 'neutral',\n",
       "  'pert': 'neutral',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9068921767175198,\n",
       "  'm1': 0.9068921767175198},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7618344500660896,\n",
       "  'm1': 0.7618344500660896},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7618344500660896,\n",
       "  'm1': 0.7618344500660896},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.7618344500660896,\n",
       "  'm1': 0.7618344500660896},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9388633407652378,\n",
       "  'm1': 0.9379394296556711},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9388633407652378,\n",
       "  'm1': 0.9376823212951422},\n",
       " {'orig': 'positive',\n",
       "  'pert': 'positive',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9388633407652378,\n",
       "  'm1': 0.9389546234160662},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9580380655825138,\n",
       "  'm1': 0.9581657871603966},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9580380655825138,\n",
       "  'm1': 0.9585818015038967},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9580380655825138,\n",
       "  'm1': 0.9579686876386404},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9396166913211346,\n",
       "  'm1': 0.9396166913211346},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9396166913211346,\n",
       "  'm1': 0.9396166913211346},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9396166913211346,\n",
       "  'm1': 0.9396166913211346},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9539002440869808,\n",
       "  'm1': 0.9539002440869808},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9539002440869808,\n",
       "  'm1': 0.9539002440869808},\n",
       " {'orig': 'negative',\n",
       "  'pert': 'negative',\n",
       "  'passed': True,\n",
       "  'flipped': False,\n",
       "  'weakened': False,\n",
       "  'm0': 0.9539002440869808,\n",
       "  'm1': 0.9540072623640299}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gen_perturb_sample(text):\n",
    "    cands = [\n",
    "        synonym_replace(text),\n",
    "        template_paraphrase_v1(text),\n",
    "        perturb_numbers_entities(text),\n",
    "    ]\n",
    "    outs = []\n",
    "    for t in cands:\n",
    "        ok, info = pass_filter(text, t)\n",
    "        if not ok: \n",
    "            outs.append({\"orig\":text,\"pert\":t,\"passed\":False, **info}); continue\n",
    "        tag = adversarial_tag(text, t)\n",
    "        outs.append({\"orig\":text,\"pert\":t,\"passed\":True, **tag})\n",
    "    return outs\n",
    "\n",
    "# 对一批金融标题生成结果表\n",
    "def build_library(headlines, K_per_type=1):\n",
    "    rows = []\n",
    "    for h in headlines:\n",
    "        rows += gen_perturb_sample(h)\n",
    "    return rows\n",
    "\n",
    "# print(X_train_.text.tolist()[:1])\n",
    "\n",
    "build_library(X_train_new.text.tolist()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40e4acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do\n",
    "# 扰动结果分析\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db7541a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) FinBERT 句向量相似度（像不像同一句话）\n",
    "# 先粘贴一版可用的接口（不用懂内部）\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_FIN = \"ProsusAI/finbert\"\n",
    "_tok = AutoTokenizer.from_pretrained(_FIN)\n",
    "_enc = AutoModel.from_pretrained(_FIN).to(_DEVICE).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _embed(text):\n",
    "    t = _tok(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(_DEVICE)\n",
    "    out = _enc(**t).last_hidden_state[:,0,:]               # 取 [CLS]\n",
    "    return F.normalize(out, p=2, dim=-1).squeeze(0)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return float(F.cosine_similarity(a[None], b[None]).item())\n",
    "\n",
    "# 2) 简单可读性（先占位：用长度限制替代，后面再换 PPL 也行）\n",
    "def readability_ok(text, max_len=80):\n",
    "    return len(text) <= max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b6ac433",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU_SEM = 0.85   # 语义相似度阈值（先用 0.85）\n",
    "def pass_filter(orig, pert):\n",
    "    sim = cosine_sim(_embed(orig), _embed(pert))\n",
    "    ok = (sim >= TAU_SEM) and readability_ok(pert)\n",
    "    return ok, {\"sim\": sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7591062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 1) FinBERT (reward judge) 概率接口 ======\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 用于“奖励”的 FinBERT（带分类头）\n",
    "FIN_CLS = \"yiyanghkust/finbert-tone\"\n",
    "fin_tok = AutoTokenizer.from_pretrained(FIN_CLS)\n",
    "fin_mdl = AutoModelForSequenceClassification.from_pretrained(FIN_CLS).to(DEVICE).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_probs(texts):\n",
    "    toks = fin_tok(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=256).to(DEVICE)\n",
    "    logits = fin_mdl(**toks).logits\n",
    "    return logits.softmax(-1)  # [B, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9a4f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEGATIVE ===\n",
      "Kept 15 tokens:\n",
      "  - 'negative'         -> id=8178   token_str='▁negative'\n",
      "  - ' BEARISH (last)'  -> id=29950  token_str='H'\n",
      "  - 'down'             -> id=1623   token_str='▁down'\n",
      "  - ' SLUMP (last)'    -> id=3580   token_str='MP'\n",
      "  - ' PLUNGE (last)'   -> id=1692   token_str='GE'\n",
      "  - 'drop'             -> id=5768   token_str='▁drop'\n",
      "  - 'fall'             -> id=6416   token_str='▁fall'\n",
      "  - 'miss'             -> id=3052   token_str='▁miss'\n",
      "  - 'lower'            -> id=5224   token_str='▁lower'\n",
      "  - ' DOWNGRADE (last)' -> id=2287   token_str='DE'\n",
      "  - 'warn'             -> id=29383  token_str='▁warn'\n",
      "  - 'loss'             -> id=6410   token_str='▁loss'\n",
      "  - ' DECLINE (last)'  -> id=8895   token_str='INE'\n",
      "  - 'weak'             -> id=8062   token_str='▁weak'\n",
      "  - ' SOFTEN (last)'   -> id=1430   token_str='EN'\n",
      "Multi-token encountered (6):\n",
      "  * ' BEARISH' -> ids=[29871, 20700, 1718, 3235, 29950] toks=['▁', '▁BE', 'AR', 'IS', 'H']\n",
      "  * ' SLUMP' -> ids=[29871, 27146, 29965, 3580] toks=['▁', '▁SL', 'U', 'MP']\n",
      "  * ' PLUNGE' -> ids=[29871, 16507, 3904, 1692] toks=['▁', '▁PL', 'UN', 'GE']\n",
      "  * ' DOWNGRADE' -> ids=[29871, 360, 9806, 9312, 4717, 2287] toks=['▁', '▁D', 'OW', 'NG', 'RA', 'DE']\n",
      "  * ' DECLINE' -> ids=[29871, 5012, 6154, 8895] toks=['▁', '▁DE', 'CL', 'INE']\n",
      "  * ' SOFTEN' -> ids=[29871, 7791, 7818, 1430] toks=['▁', '▁SO', 'FT', 'EN']\n",
      "\n",
      "=== NEUTRAL ===\n",
      "Kept 10 tokens:\n",
      "  - 'neutral'          -> id=21104  token_str='▁neutral'\n",
      "  - ' UNCHANGED (last)' -> id=29928  token_str='D'\n",
      "  - 'flat'             -> id=12151  token_str='▁flat'\n",
      "  - 'steady'           -> id=27357  token_str='▁steady'\n",
      "  - 'stable'           -> id=13714  token_str='▁stable'\n",
      "  - 'mixed'            -> id=12849  token_str='▁mixed'\n",
      "  - 'inline'           -> id=10583  token_str='▁inline'\n",
      "  - ' RANGEBOUND (last)' -> id=18783  token_str='UND'\n",
      "  - ' MUTED (last)'    -> id=3352   token_str='ED'\n",
      "  - ' SIDEWAYS (last)' -> id=21554  token_str='YS'\n",
      "Multi-token encountered (4):\n",
      "  * ' UNCHANGED' -> ids=[29871, 8291, 3210, 24336, 29928] toks=['▁', '▁UN', 'CH', 'ANGE', 'D']\n",
      "  * ' RANGEBOUND' -> ids=[29871, 390, 24336, 8456, 18783] toks=['▁', '▁R', 'ANGE', 'BO', 'UND']\n",
      "  * ' MUTED' -> ids=[29871, 341, 2692, 3352] toks=['▁', '▁M', 'UT', 'ED']\n",
      "  * ' SIDEWAYS' -> ids=[29871, 317, 22027, 12982, 21554] toks=['▁', '▁S', 'IDE', 'WA', 'YS']\n",
      "\n",
      "=== POSITIVE ===\n",
      "Kept 15 tokens:\n",
      "  - 'positive'         -> id=6374   token_str='▁positive'\n",
      "  - ' BULLISH (last)'  -> id=29950  token_str='H'\n",
      "  - 'up'               -> id=701    token_str='▁up'\n",
      "  - ' RALLY (last)'    -> id=29979  token_str='Y'\n",
      "  - ' SURGE (last)'    -> id=1692   token_str='GE'\n",
      "  - 'jump'             -> id=12500  token_str='▁jump'\n",
      "  - ' SOAR (last)'     -> id=1718   token_str='AR'\n",
      "  - 'beat'             -> id=16646  token_str='▁beat'\n",
      "  - 'raise'            -> id=12020  token_str='▁raise'\n",
      "  - 'upgrade'          -> id=14955  token_str='▁upgrade'\n",
      "  - 'profit'           -> id=21665  token_str='▁profit'\n",
      "  - 'growth'           -> id=14321  token_str='▁growth'\n",
      "  - 'expand'           -> id=7985   token_str='▁expand'\n",
      "  - 'strong'           -> id=4549   token_str='▁strong'\n",
      "  - ' OUTPERFORM (last)' -> id=19094  token_str='FORM'\n",
      "Multi-token encountered (5):\n",
      "  * ' BULLISH' -> ids=[29871, 350, 3299, 3235, 29950] toks=['▁', '▁B', 'ULL', 'IS', 'H']\n",
      "  * ' RALLY' -> ids=[29871, 390, 9818, 29979] toks=['▁', '▁R', 'ALL', 'Y']\n",
      "  * ' SURGE' -> ids=[29871, 317, 4574, 1692] toks=['▁', '▁S', 'UR', 'GE']\n",
      "  * ' SOAR' -> ids=[29871, 7791, 1718] toks=['▁', '▁SO', 'AR']\n",
      "  * ' OUTPERFORM' -> ids=[29871, 19474, 13171, 19094] toks=['▁', '▁OUT', 'PER', 'FORM']\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Verbalizer 体检与清洗工具\n",
    "# 依赖：只需要已经就绪的 `tokenizer`\n",
    "# =========================\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import re\n",
    "\n",
    "def tokenize_no_special(text: str, tokenizer) -> List[int]:\n",
    "    \"\"\"对字符串做分词，返回 token id 列表（不加特殊符号）。\"\"\"\n",
    "    return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "def inspect_tokens(words: List[str], tokenizer) -> List[Tuple[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    对每个词做体检，返回 (词, token_id_list)。\n",
    "    建议传入带前导空格的词（如 ' positive'），更易单 token。\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    for w in words:\n",
    "        ids = tokenize_no_special(w, tokenizer)\n",
    "        report.append((w, ids))\n",
    "    return report\n",
    "\n",
    "def try_variants_for_single_token(word: str, tokenizer, max_variants: int = 6) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    自动尝试若干“变体”，力争找出一个单 token 的版本。\n",
    "    返回 (选中的词形, token_ids)。若都不行，返回最后一次的 ids。\n",
    "    变体规则可按需扩展。\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    w = word\n",
    "    # 常用变体：加/去前导空格、首字母小写、大写、带连字符等\n",
    "    variants = [\n",
    "        w, \n",
    "        \" \" + w.lstrip(), \n",
    "        w.strip(), \n",
    "        w.lower() if w != w.lower() else w,\n",
    "        w.upper() if w != w.upper() else w,\n",
    "        \" \" + w.lower().lstrip(),\n",
    "        \" \" + re.sub(r\"\\s+\", \"-\", w.strip()),  # 空格->连字符\n",
    "    ]\n",
    "    # 去重保序\n",
    "    seen = set(); uniq_variants = []\n",
    "    for v in variants:\n",
    "        if v not in seen:\n",
    "            uniq_variants.append(v); seen.add(v)\n",
    "\n",
    "    for v in uniq_variants[:max_variants]:\n",
    "        ids = tokenize_no_special(v, tokenizer)\n",
    "        candidates.append((v, ids))\n",
    "        if len(ids) == 1:\n",
    "            return v, ids\n",
    "    # 都不是单 token，则返回最后一个尝试\n",
    "    return candidates[-1]\n",
    "\n",
    "def build_verbalizers(\n",
    "    seed_dict: Dict[str, List[str]], \n",
    "    tokenizer, \n",
    "    prefer_single_token: bool = True, \n",
    "    allow_last_token_fallback: bool = True,\n",
    "    auto_variant_search: bool = True,\n",
    ") -> Dict[str, Dict[str, List]]:\n",
    "    \"\"\"\n",
    "    清洗入口：\n",
    "    - 优先保留单 token；\n",
    "    - 若允许回退，把多 token 词用“最后一个 token”近似；\n",
    "    - 去重同一 token id；\n",
    "    - 输出 {label: {\"tokens\": [ids...], \"words\": [对应词...], \"dropped\": [...], \"multi_token\": [...]} }\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for label, words in seed_dict.items():\n",
    "        kept_tokens, kept_words = [], []\n",
    "        dropped, multi_info = [], []\n",
    "        seen_ids = set()\n",
    "\n",
    "        for w in words:\n",
    "            if auto_variant_search:\n",
    "                w_chosen, ids = try_variants_for_single_token(w, tokenizer)\n",
    "            else:\n",
    "                w_chosen, ids = w, tokenize_no_special(w, tokenizer)\n",
    "\n",
    "            if len(ids) == 1:\n",
    "                tid = ids[0]\n",
    "                if tid not in seen_ids:\n",
    "                    kept_tokens.append(tid); kept_words.append(w_chosen); seen_ids.add(tid)\n",
    "                else:\n",
    "                    dropped.append((w_chosen, ids, \"dup_token\"))\n",
    "            else:\n",
    "                # 多 token 情况\n",
    "                multi_info.append((w_chosen, ids))\n",
    "                if prefer_single_token:\n",
    "                    if allow_last_token_fallback and len(ids) > 0:\n",
    "                        tid = ids[-1]\n",
    "                        if tid not in seen_ids:\n",
    "                            kept_tokens.append(tid); kept_words.append(w_chosen+\" (last)\"); seen_ids.add(tid)\n",
    "                        else:\n",
    "                            dropped.append((w_chosen, ids, \"dup_last_token\"))\n",
    "                    else:\n",
    "                        dropped.append((w_chosen, ids, \"multi_token_dropped\"))\n",
    "                else:\n",
    "                    # 不强制单 token，直接聚合多 token：这里通常不建议\n",
    "                    # 若一定要，改为 kept_tokens.extend(ids) 并在聚合时特殊处理\n",
    "                    dropped.append((w_chosen, ids, \"multi_token_ignored\"))\n",
    "\n",
    "        out[label] = {\n",
    "            \"tokens\": kept_tokens,\n",
    "            \"words\": kept_words,\n",
    "            \"dropped\": dropped,       # 被丢弃或因重复删除的\n",
    "            \"multi_token\": multi_info # 原始多 token 情况备案\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def pretty_print_verbalizer_report(verb_clean: Dict[str, Dict], tokenizer, label_order=None, max_show=999):\n",
    "    \"\"\"打印清洗结果与诊断信息。\"\"\"\n",
    "    if label_order is None: label_order = list(verb_clean.keys())\n",
    "    for label in label_order:\n",
    "        blk = verb_clean[label]\n",
    "        print(f\"\\n=== {label.upper()} ===\")\n",
    "        print(f\"Kept {len(blk['tokens'])} tokens:\")\n",
    "        for w, tid in zip(blk[\"words\"][:max_show], blk[\"tokens\"][:max_show]):\n",
    "            print(f\"  - {w!r:<18} -> id={tid:<6} token_str={tokenizer.convert_ids_to_tokens([tid])[0]!r}\")\n",
    "        if blk[\"multi_token\"]:\n",
    "            print(f\"Multi-token encountered ({len(blk['multi_token'])}):\")\n",
    "            for w, ids in blk[\"multi_token\"][:max_show]:\n",
    "                toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "                print(f\"  * {w!r} -> ids={ids} toks={toks}\")\n",
    "        if blk[\"dropped\"]:\n",
    "            print(f\"Dropped ({len(blk['dropped'])}):\")\n",
    "            for w, ids, reason in blk[\"dropped\"][:max_show]:\n",
    "                print(f\"  x {w!r} -> {reason}  ids={ids}\")\n",
    "\n",
    "# ===== 示例：把你的初始词表丢进来体检清洗 =====\n",
    "SEEDS = {\n",
    "    \"negative\": [\" negative\",\" bearish\",\" down\",\" slump\",\" plunge\",\" drop\",\" fall\",\" miss\",\n",
    "                 \" lower\",\" downgrade\",\" warn\",\" loss\",\" decline\",\" weak\",\" soften\"],\n",
    "    \"neutral\":  [\" neutral\",\" unchanged\",\" flat\",\" steady\",\" stable\",\" mixed\",\" inline\",\" rangebound\",\n",
    "                 \" muted\",\" sideways\"],\n",
    "    \"positive\": [\" positive\",\" bullish\",\" up\",\" rally\",\" surge\",\" jump\",\" soar\",\" beat\",\n",
    "                 \" raise\",\" upgrade\",\" profit\",\" growth\",\" expand\",\" strong\",\" outperform\"],\n",
    "}\n",
    "\n",
    "# 跑清洗（注意：这里用的是你已经加载好的 LLaMA tokenizer）\n",
    "VERB = build_verbalizers(\n",
    "    SEEDS, tokenizer,\n",
    "    prefer_single_token=True, \n",
    "    allow_last_token_fallback=True,\n",
    "    auto_variant_search=True\n",
    ")\n",
    "\n",
    "\n",
    "# 运行完你会看到一份清晰的报告：\n",
    "\n",
    "# Kept：最终参与聚合的 token ids 与对应词形（后缀 “(last)” 表示走了“最后 token 回退”）。\n",
    "\n",
    "# Multi-token：原始被拆的词及其 token 列表，便于你手工替换同义词。\n",
    "\n",
    "# Dropped：因为重复 token 或策略丢弃的词。\n",
    "pretty_print_verbalizer_report(VERB, tokenizer, label_order=[\"negative\",\"neutral\",\"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81a97375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NEGATIVE ===\n",
      "Kept 15 tokens:\n",
      "  - 'negative'         -> id=8178   token_str='▁negative'\n",
      "  - ' BEARISH (last)'  -> id=29950  token_str='H'\n",
      "  - 'down'             -> id=1623   token_str='▁down'\n",
      "  - ' SLUMP (last)'    -> id=3580   token_str='MP'\n",
      "  - ' PLUNGE (last)'   -> id=1692   token_str='GE'\n",
      "  - 'drop'             -> id=5768   token_str='▁drop'\n",
      "  - 'fall'             -> id=6416   token_str='▁fall'\n",
      "  - 'miss'             -> id=3052   token_str='▁miss'\n",
      "  - 'lower'            -> id=5224   token_str='▁lower'\n",
      "  - ' DOWNGRADE (last)' -> id=2287   token_str='DE'\n",
      "  - 'warn'             -> id=29383  token_str='▁warn'\n",
      "  - 'loss'             -> id=6410   token_str='▁loss'\n",
      "  - ' DECLINE (last)'  -> id=8895   token_str='INE'\n",
      "  - 'weak'             -> id=8062   token_str='▁weak'\n",
      "  - ' SOFTEN (last)'   -> id=1430   token_str='EN'\n",
      "Multi-token encountered (6):\n",
      "  * ' BEARISH' -> ids=[29871, 20700, 1718, 3235, 29950] toks=['▁', '▁BE', 'AR', 'IS', 'H']\n",
      "  * ' SLUMP' -> ids=[29871, 27146, 29965, 3580] toks=['▁', '▁SL', 'U', 'MP']\n",
      "  * ' PLUNGE' -> ids=[29871, 16507, 3904, 1692] toks=['▁', '▁PL', 'UN', 'GE']\n",
      "  * ' DOWNGRADE' -> ids=[29871, 360, 9806, 9312, 4717, 2287] toks=['▁', '▁D', 'OW', 'NG', 'RA', 'DE']\n",
      "  * ' DECLINE' -> ids=[29871, 5012, 6154, 8895] toks=['▁', '▁DE', 'CL', 'INE']\n",
      "  * ' SOFTEN' -> ids=[29871, 7791, 7818, 1430] toks=['▁', '▁SO', 'FT', 'EN']\n",
      "\n",
      "=== NEUTRAL ===\n",
      "Kept 10 tokens:\n",
      "  - 'neutral'          -> id=21104  token_str='▁neutral'\n",
      "  - ' UNCHANGED (last)' -> id=29928  token_str='D'\n",
      "  - 'flat'             -> id=12151  token_str='▁flat'\n",
      "  - 'steady'           -> id=27357  token_str='▁steady'\n",
      "  - 'stable'           -> id=13714  token_str='▁stable'\n",
      "  - 'mixed'            -> id=12849  token_str='▁mixed'\n",
      "  - 'inline'           -> id=10583  token_str='▁inline'\n",
      "  - ' RANGEBOUND (last)' -> id=18783  token_str='UND'\n",
      "  - ' MUTED (last)'    -> id=3352   token_str='ED'\n",
      "  - ' SIDEWAYS (last)' -> id=21554  token_str='YS'\n",
      "Multi-token encountered (4):\n",
      "  * ' UNCHANGED' -> ids=[29871, 8291, 3210, 24336, 29928] toks=['▁', '▁UN', 'CH', 'ANGE', 'D']\n",
      "  * ' RANGEBOUND' -> ids=[29871, 390, 24336, 8456, 18783] toks=['▁', '▁R', 'ANGE', 'BO', 'UND']\n",
      "  * ' MUTED' -> ids=[29871, 341, 2692, 3352] toks=['▁', '▁M', 'UT', 'ED']\n",
      "  * ' SIDEWAYS' -> ids=[29871, 317, 22027, 12982, 21554] toks=['▁', '▁S', 'IDE', 'WA', 'YS']\n",
      "\n",
      "=== POSITIVE ===\n",
      "Kept 15 tokens:\n",
      "  - 'positive'         -> id=6374   token_str='▁positive'\n",
      "  - ' BULLISH (last)'  -> id=29950  token_str='H'\n",
      "  - 'up'               -> id=701    token_str='▁up'\n",
      "  - ' RALLY (last)'    -> id=29979  token_str='Y'\n",
      "  - ' SURGE (last)'    -> id=1692   token_str='GE'\n",
      "  - 'jump'             -> id=12500  token_str='▁jump'\n",
      "  - ' SOAR (last)'     -> id=1718   token_str='AR'\n",
      "  - 'beat'             -> id=16646  token_str='▁beat'\n",
      "  - 'raise'            -> id=12020  token_str='▁raise'\n",
      "  - 'upgrade'          -> id=14955  token_str='▁upgrade'\n",
      "  - 'profit'           -> id=21665  token_str='▁profit'\n",
      "  - 'growth'           -> id=14321  token_str='▁growth'\n",
      "  - 'expand'           -> id=7985   token_str='▁expand'\n",
      "  - 'strong'           -> id=4549   token_str='▁strong'\n",
      "  - ' OUTPERFORM (last)' -> id=19094  token_str='FORM'\n",
      "Multi-token encountered (5):\n",
      "  * ' BULLISH' -> ids=[29871, 350, 3299, 3235, 29950] toks=['▁', '▁B', 'ULL', 'IS', 'H']\n",
      "  * ' RALLY' -> ids=[29871, 390, 9818, 29979] toks=['▁', '▁R', 'ALL', 'Y']\n",
      "  * ' SURGE' -> ids=[29871, 317, 4574, 1692] toks=['▁', '▁S', 'UR', 'GE']\n",
      "  * ' SOAR' -> ids=[29871, 7791, 1718] toks=['▁', '▁SO', 'AR']\n",
      "  * ' OUTPERFORM' -> ids=[29871, 19474, 13171, 19094] toks=['▁', '▁OUT', 'PER', 'FORM']\n"
     ]
    }
   ],
   "source": [
    "# step 1: 构造/清洗 verbalizers（你前面已经跑完也行）\n",
    "SEEDS = {\n",
    "    \"negative\": [\" negative\",\" bearish\",\" down\",\" slump\",\" plunge\",\" drop\",\" fall\",\" miss\",\n",
    "                 \" lower\",\" downgrade\",\" warn\",\" loss\",\" decline\",\" weak\",\" soften\"],\n",
    "    \"neutral\":  [\" neutral\",\" unchanged\",\" flat\",\" steady\",\" stable\",\" mixed\",\" inline\",\" rangebound\",\n",
    "                 \" muted\",\" sideways\"],\n",
    "    \"positive\": [\" positive\",\" bullish\",\" up\",\" rally\",\" surge\",\" jump\",\" soar\",\" beat\",\n",
    "                 \" raise\",\" upgrade\",\" profit\",\" growth\",\" expand\",\" strong\",\" outperform\"],\n",
    "}\n",
    "\n",
    "# 直接使用你之前提供的 build_verbalizers / pretty_print_verbalizer_report\n",
    "VERB = build_verbalizers(\n",
    "    SEEDS, tokenizer,\n",
    "    prefer_single_token=True,\n",
    "    allow_last_token_fallback=True,\n",
    "    auto_variant_search=True\n",
    ")\n",
    "pretty_print_verbalizer_report(VERB, tokenizer, label_order=[\"negative\",\"neutral\",\"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c066d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_ORDER = [\"negative\",\"neutral\",\"positive\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def probs_from_causallm(texts, tokenizer, causal_lm, VERB, label_order=LABEL_ORDER, max_length=256):\n",
    "    causal_lm.eval()  # 推理更稳\n",
    "    # 1) 构造 prompt（你也可以换成自己的分类prompt或chat模板）\n",
    "    prompts = [f\"Text: {t}\\nSentiment:\" for t in texts]\n",
    "\n",
    "    # 2) 编码（不要手动 .to(cuda)，交给 HF 处理；对 device_map='auto' 兼容）\n",
    "    toks = tokenizer(prompts, padding=True, truncation=True,\n",
    "                     return_tensors=\"pt\", max_length=max_length)\n",
    "    \n",
    "    toks = _to_model_device(toks, model)\n",
    "\n",
    "    # 3) 前向\n",
    "    out  = causal_lm(**toks)\n",
    "\n",
    "    # 4) 取“下一个词”的 logits（最后一个非 pad 位置之后的预测）\n",
    "    last_idx    = (toks[\"attention_mask\"].sum(dim=1) - 1)                 # [B]\n",
    "    logits_last = out.logits[torch.arange(out.logits.size(0)), last_idx]  # [B, V]\n",
    "\n",
    "    # 5) 用 VERB 聚合到三类分数（logsumexp），再 softmax 得到概率\n",
    "    scores = []\n",
    "    for label in label_order:\n",
    "        ids = VERB[label][\"tokens\"]\n",
    "        if len(ids) == 0:\n",
    "            sc = torch.full((logits_last.size(0),), -1e9, dtype=logits_last.dtype, device=logits_last.device)\n",
    "        elif len(ids) == 1:\n",
    "            sc = logits_last[:, ids[0]]\n",
    "        else:\n",
    "            sc = torch.logsumexp(logits_last[:, ids], dim=1)\n",
    "        scores.append(sc)\n",
    "    scores = torch.stack(scores, dim=1)   # [B, 3]\n",
    "    probs  = scores.softmax(dim=1)        # [B, 3] -> 顺序=LABEL_ORDER\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f92d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0174, 0.0128, 0.9698],\n",
      "        [0.9776, 0.0166, 0.0058],\n",
      "        [0.4102, 0.0610, 0.5287]], device='cuda:0')\n",
      "pred: [2, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Apple shares rallied after beating earnings expectations.\",\n",
    "    \"The company issued a profit warning and the stock fell.\",\n",
    "    \"Indexes were little changed in thin trading.\"\n",
    "]\n",
    "\n",
    "p_x = probs_from_causallm(texts, tokenizer, model, VERB)   # [B,3], 顺序[neg, neu, pos]\n",
    "print(p_x)\n",
    "print(\"pred:\", p_x.argmax(dim=-1).tolist())  # 2->positive, 0->negative, 1->neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d33eab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你可以按需要微调词表\n",
    "LABEL_ORDER = [\"negative\", \"neutral\", \"positive\"]  # 统一顺序\n",
    "\n",
    "\n",
    "class StudentClassifierForCausalLM(nn.Module):\n",
    "    \"\"\"\n",
    "    把 LLaMA CausalLM + tokenizer 包装成三分类“概率模型”（不改你原模型结构）\n",
    "    \"\"\"\n",
    "    def __init__(self, causal_lm, tokenizer, verbalizers=VERB, max_length=256, use_chat_template=False):\n",
    "        super().__init__()\n",
    "        self.causal_lm = causal_lm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_chat_template = use_chat_template\n",
    "\n",
    "        # 预编码 verbalizer -> token ids（尽量取单 token；若多 token，取最后一个作为近似）\n",
    "        self.label_token_ids = {}\n",
    "        for label, words in verbalizers.items():\n",
    "            ids = []\n",
    "            for w in words:\n",
    "                toks = tokenizer.encode(w, add_special_tokens=False)\n",
    "                if len(toks) == 1:\n",
    "                    ids.append(toks[0])\n",
    "                elif len(toks) > 1:\n",
    "                    ids.append(toks[-1])  # 退路：取最后一个 token 聚合\n",
    "            # 去重\n",
    "            ids = list(sorted(set(ids)))\n",
    "            if not ids:\n",
    "                # 再保底一个常见 token，避免空\n",
    "                ids = tokenizer.encode(\" positive\", add_special_tokens=False)[-1:]\n",
    "            self.label_token_ids[label] = ids\n",
    "\n",
    "    def _build_prompts(self, texts):\n",
    "        # 与你之前 prompt 逻辑类似：让下一个词直接就是情感词\n",
    "        # 若你用了 chat 模板 (setup_chat_format)，可以切换 use_chat_template=True\n",
    "        if not self.use_chat_template:\n",
    "            return [f\"Text: {t}\\nSentiment:\" for t in texts]\n",
    "        else:\n",
    "            prompts = []\n",
    "            for t in texts:\n",
    "                msgs = [{\"role\":\"user\",\"content\":f\"Classify the sentiment of the following text into negative, neutral, or positive.\\nText: {t}\\nAnswer one word: Sentiment:\"}]\n",
    "                prompts.append(self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "            return prompts\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, texts):\n",
    "        self.causal_lm.eval()\n",
    "        prompts = self._build_prompts(texts)\n",
    "\n",
    "        toks = self.tokenizer(\n",
    "            prompts, padding=True, truncation=True,\n",
    "            return_tensors=\"pt\", max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        toks = _to_model_device(toks, self.causal_lm)  # 兼容 device_map='auto'\n",
    "\n",
    "        out = self.causal_lm(**toks)\n",
    "        # 取“下一个词”的 logits\n",
    "        last_idx = (toks[\"attention_mask\"].sum(dim=1) - 1)  # [B]\n",
    "        logits_last = out.logits[torch.arange(out.logits.size(0)), last_idx, :]  # [B, V]\n",
    "\n",
    "        # 把若干 verbalizer token 聚合成 3 类得分（logsumexp）\n",
    "        label_scores = []\n",
    "        for label in LABEL_ORDER:\n",
    "            ids = self.label_token_ids[label]\n",
    "            if len(ids) == 1:\n",
    "                score = logits_last[:, ids[0]]\n",
    "            else:\n",
    "                score = torch.logsumexp(logits_last[:, ids], dim=1)\n",
    "            label_scores.append(score)\n",
    "        scores = torch.stack(label_scores, dim=1)  # [B, 3]\n",
    "        probs  = scores.softmax(dim=1)\n",
    "        return probs  # [B,3]\n",
    "\n",
    "\n",
    "\n",
    "# 直接用你已有的 `model` 和 `tokenizer`\n",
    "student_model = StudentClassifierForCausalLM(\n",
    "    causal_lm = model,\n",
    "    tokenizer = tokenizer,\n",
    "    verbalizers = VERB,\n",
    "    max_length = 256,\n",
    "    use_chat_template = False  # 如果你的 setup_chat_format 必须走 chat 模板，就改成 True\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def student_probs(texts):\n",
    "    return student_model.forward(texts)  # -> [B,3]\n",
    "\n",
    "def predict_by_probs(texts, batch_size=64):\n",
    "    y_pred = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        p = student_probs(batch)                  # [B,3]\n",
    "        idx = p.argmax(dim=-1).tolist()\n",
    "        for j in idx:\n",
    "            y_pred.append([\"negative\",\"neutral\",\"positive\"][j])\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13ebfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_ = fin_mdl.to(DEVICE).eval()\n",
    "\n",
    "# 把 finbert 输出列顺序对齐到 LABEL_ORDER\n",
    "FIN_ID2LABEL = {int(k): v.lower() for k, v in fin_mdl.config.id2label.items()}  # e.g., {0:'positive',1:'negative',2:'neutral'}\n",
    "FIN_LABEL2IDX = {v: k for k, v in FIN_ID2LABEL.items()}\n",
    "\n",
    "def _finbert_reorder_indices(label_order):\n",
    "    # 返回一个 index 列表，用于把 q[:, finbert_idx] 变成 q[:, label_order_idx]\n",
    "    idxs = []\n",
    "    for lab in label_order:\n",
    "        if lab not in FIN_LABEL2IDX:\n",
    "            raise ValueError(f\"FinBERT没有这个label: {lab}. 现有: {list(FIN_LABEL2IDX.keys())}\")\n",
    "        idxs.append(FIN_LABEL2IDX[lab])\n",
    "    return torch.tensor(idxs, device=DEVICE)\n",
    "\n",
    "FIN_REORDER_IDX = _finbert_reorder_indices(LABEL_ORDER)  # shape [3]\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_probs(texts):\n",
    "    toks = fin_tokenizer(texts, padding=True, truncation=True,\n",
    "                         return_tensors=\"pt\", max_length=256).to(DEVICE)\n",
    "    logits = fin_mdl(**toks).logits            # [B, 3]（FinBERT的自身顺序）\n",
    "    q = logits.softmax(-1)\n",
    "    # 重排到你的 LABEL_ORDER\n",
    "    q = q.index_select(dim=1, index=FIN_REORDER_IDX)  # [B, 3]，顺序与 LABEL_ORDER 一致\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47387d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo 奖励函数 READ\n",
    "\n",
    "def _sym_kl(p, q):\n",
    "    p = p.clamp_min(1e-12); q = q.clamp_min(1e-12)\n",
    "    return (F.kl_div(p.log(), q, reduction=\"none\").sum(-1) +\n",
    "            F.kl_div(q.log(), p, reduction=\"none\").sum(-1))\n",
    "\n",
    "def consistency_reward(p_x, p_xt):\n",
    "    # D：x 与 x~ 的分布要接近 → 负对称KL当奖励\n",
    "    return - _sym_kl(p_x, p_xt)\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_align_reward(texts, p_x):\n",
    "    # C2：学生分布贴近 FinBERT（判官）\n",
    "    q_x = finbert_probs(texts)   # 已对齐到 LABEL_ORDER\n",
    "    q_x = q_x.clamp_min(1e-12); p = p_x.clamp_min(1e-12)\n",
    "    return 1.0 - F.kl_div(q_x.log(), p, reduction=\"none\").sum(-1)\n",
    "\n",
    "def calibration_reward(p_x):\n",
    "    # 轻量置信度正则（-Entropy）\n",
    "    p = p_x.clamp_min(1e-12)\n",
    "    return (p * p.log()).sum(-1)\n",
    "\n",
    "W_CONS, W_FIN, W_CAL = 1.0, 0.5, 0.01\n",
    "\n",
    "def total_reward(p_x, p_xt, texts):\n",
    "    return (W_CONS * consistency_reward(p_x, p_xt)\n",
    "            + W_FIN  * finbert_align_reward(texts, p_x)\n",
    "            + W_CAL  * calibration_reward(p_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1517c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pick_one_pert(text):\n",
    "    cands = gen_perturb_sample(text)        # 你的函数：返回多个候选及 passed 标志\n",
    "    passed = [c for c in cands if c.get(\"passed\", False)]\n",
    "    if not passed: \n",
    "        return None\n",
    "    return random.choice(passed)[\"pert\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9e097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 448 parameters.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "EPS_CLIP   = 0.1\n",
    "LR         = 1e-5\n",
    "KL_COEF    = 0.01\n",
    "EMA_ALPHA  = 0.9\n",
    "baseline_ema = 0.0\n",
    "\n",
    "# 只优化可训练参数（如果你挂了 LoRA，只会包含 LoRA 的参数）\n",
    "trainable_params = [p for p in student_model.causal_lm.parameters() if p.requires_grad]\n",
    "print( f\"Training {len(trainable_params)} parameters.\" ) # 0\n",
    "optimizer = AdamW(trainable_params, lr=LR)\n",
    "\n",
    "def _batched_probs_with_grad(texts):\n",
    "    \"\"\"在训练图里取 p(x) 概率；student_model 内部会用 causal_lm 前向\"\"\"\n",
    "    student_model.causal_lm.train()\n",
    "    # 用包装器直接拿 [B,3] 概率（带梯度）\n",
    "    # 取消 no_grad：我们需要反向\n",
    "    prompts = student_model._build_prompts(texts)\n",
    "    toks = student_model.tokenizer(\n",
    "        prompts, padding=True, truncation=True,\n",
    "        return_tensors=\"pt\", max_length=student_model.max_length\n",
    "    )\n",
    "    toks = _to_model_device(toks, student_model.causal_lm)\n",
    "    out = student_model.causal_lm(**toks)\n",
    "    last_idx = (toks[\"attention_mask\"].sum(dim=1) - 1)\n",
    "    logits_last = out.logits[torch.arange(out.logits.size(0)), last_idx, :]\n",
    "    label_scores = []\n",
    "    for label in LABEL_ORDER:\n",
    "        ids = student_model.label_token_ids[label]\n",
    "        if len(ids) == 1:\n",
    "            score = logits_last[:, ids[0]]\n",
    "        else:\n",
    "            score = torch.logsumexp(logits_last[:, ids], dim=1)\n",
    "        label_scores.append(score)\n",
    "    scores = torch.stack(label_scores, dim=1)\n",
    "    probs  = scores.softmax(dim=1)  # [B,3]\n",
    "    # 选动作（贪心），并得到 logp(a|x)\n",
    "    actions = probs.argmax(dim=-1)\n",
    "    logp = (probs.clamp_min(1e-12).log().gather(1, actions[:,None]).squeeze(1))\n",
    "    return probs, actions, logp\n",
    "\n",
    "@torch.no_grad()\n",
    "def _batched_probs_old(texts):\n",
    "    student_model.causal_lm.eval()\n",
    "    p = student_probs(texts)  # 你的无梯度接口\n",
    "    actions = p.argmax(dim=-1)\n",
    "    logp = (p.clamp_min(1e-12).log().gather(1, actions[:,None]).squeeze(1))\n",
    "    return p, actions, logp\n",
    "\n",
    "def grpo_train_step(batch_texts):\n",
    "    global baseline_ema\n",
    "\n",
    "    # 1) 构造成对 (x, x̃)\n",
    "    texts, texts_tilde = [], []\n",
    "    for t in batch_texts:\n",
    "        pt = pick_one_pert(t)\n",
    "        if pt is None:\n",
    "            continue\n",
    "        texts.append(t)\n",
    "        texts_tilde.append(pt)\n",
    "    if not texts:\n",
    "        return {\"skipped\": True}\n",
    "\n",
    "    # 2) 旧策略\n",
    "    p_old, _, logp_old = _batched_probs_old(texts)\n",
    "\n",
    "    # 3) 当前策略（带梯度）\n",
    "    p_new, _, logp_new = _batched_probs_with_grad(texts)\n",
    "    p_xt, _, _         = _batched_probs_with_grad(texts_tilde)\n",
    "\n",
    "    # 4) 奖励 & 优势（奖励不回传）\n",
    "    with torch.no_grad():\n",
    "        R = total_reward(p_new.detach(), p_xt.detach(), texts)  # [B]\n",
    "        baseline_ema = EMA_ALPHA*baseline_ema + (1-EMA_ALPHA)*R.mean().item()\n",
    "        adv = R - baseline_ema\n",
    "\n",
    "    # 5) PPO/GRPO 裁剪目标\n",
    "    ratio = torch.exp(logp_new - logp_old)\n",
    "    unclipped = - ratio * adv\n",
    "    clipped   = - torch.clamp(ratio, 1.0-EPS_CLIP, 1.0+EPS_CLIP) * adv\n",
    "    loss_policy = torch.maximum(unclipped, clipped).mean()\n",
    "\n",
    "    # 6) 稳定项：新旧分布 KL 正则\n",
    "    kl_reg = F.kl_div(p_old.clamp_min(1e-12).log(), p_new.clamp_min(1e-12), reduction=\"batchmean\")\n",
    "    loss = loss_policy + KL_COEF * kl_reg\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        flip_rate = (p_new.argmax(-1) != p_xt.argmax(-1)).float().mean().item()\n",
    "\n",
    "    return {\"loss\": float(loss.item()), \"R\": float(R.mean().item()), \"flip\": flip_rate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "def run_grpo(train_texts, batch_size=32, steps=200):\n",
    "    for step in trange(steps):\n",
    "        batch = random.sample(train_texts, k=min(batch_size, len(train_texts)))\n",
    "        stats = grpo_train_step(batch)\n",
    "        if stats.get(\"skipped\"): \n",
    "            continue\n",
    "        if step % 10 == 0:\n",
    "            print(f\"[{step}] loss={stats['loss']:.4f}  R={stats['R']:.3f}  flip={stats['flip']:.3f}\")\n",
    "\n",
    "# 评估（用你现有的预测接口已换成概率版）\n",
    "def evaluate_macro_f1(texts, labels, batch_size=64):\n",
    "    from sklearn.metrics import f1_score\n",
    "    preds = predict_by_probs(texts, batch_size=batch_size)   # 使用上面写好的概率->argmax\n",
    "    return f1_score(labels, preds, labels=[\"negative\",\"neutral\",\"positive\"], average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da3a6238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<08:29,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss=1.8453  R=-2.050  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:26<07:28,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:50<07:04,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [01:13<06:39,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [01:37<06:16,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [02:01<05:54,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [02:24<05:30,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 71/200 [02:48<05:05,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [03:12<04:41,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 91/200 [03:35<04:18,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [03:59<03:55,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 111/200 [04:23<03:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [04:46<03:07,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/200 [05:10<02:43,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [05:34<02:19,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 151/200 [05:58<01:56,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 161/200 [06:21<01:32,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 171/200 [06:45<01:08,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 181/200 [07:09<00:45,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 191/200 [07:32<00:21,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190] loss=nan  R=nan  flip=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:54<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1 after GRPO: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "# E1) 准备训练/验证集（把列名改成你实际的）\n",
    "train_texts = X_train_new[\"text\"].astype(str).tolist()\n",
    "val_texts   = X_eval_new[\"text\"].astype(str).tolist()\n",
    "val_labels  = X_eval_new[\"sentiment\"].astype(str).tolist()\n",
    "\n",
    "# E2) 先跑一个 sanity 小步\n",
    "run_grpo(train_texts, batch_size=32, steps=200)\n",
    "\n",
    "# E3) 评估（用你的概率预测接口）\n",
    "print(\"Macro-F1 after GRPO:\", evaluate_macro_f1(val_texts, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b42e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
